{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HighRes_Patch_Ganomaly_v2-Copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diviramon/BASSic/blob/master/HighRes_Patch_Ganomaly_v2_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPy0KI0MNEBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3592e001-7894-4be5-8b80-6cbf7770a2bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfl7GGZDkDQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984b4d70-2e11-4acc-bab2-e8a93fac3e22"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 15 17:51:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1K3j6ZGLk0v"
      },
      "source": [
        "# Import Libraries and Create Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR3A8XXuSp1B",
        "outputId": "418ffe7a-3f0b-46a4-d838-9988681b15d4"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 13.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5blDQTnbtK5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6861d91-62f5-47f4-a51f-acad2dcccc57"
      },
      "source": [
        "!pip install pytorch-msssim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-msssim\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/d3/3cb0f397232cf79e1762323c3a8862e39ad53eca0bb5f6be9ccc8e7c070e/pytorch_msssim-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-msssim) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-msssim) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-msssim) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-msssim) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-msssim) (3.7.4.3)\n",
            "Installing collected packages: pytorch-msssim\n",
            "Successfully installed pytorch-msssim-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEmluJh7Pknz"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import random\n",
        "from skimage.io import imread \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.autograd import grad\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import random_split, ConcatDataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, f1_score\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pydicom as dcm\n",
        "from skimage import io, exposure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZG41HfCPpqX"
      },
      "source": [
        "!rm -r sample_data/\n",
        "!mkdir data\n",
        "!mkdir data/train\n",
        "!mkdir data/train/healthy\n",
        "!mkdir data/valid\n",
        "!mkdir data/valid/healthy\n",
        "!mkdir data/valid/nodule\n",
        "!mkdir data/test/\n",
        "!mkdir data/test/healthy\n",
        "!mkdir data/test/nodule\n",
        "!mkdir data/patients/\n",
        "!mkdir output/\n",
        "!mkdir output/patients/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6ER_E82LdgE"
      },
      "source": [
        "# Read Image Functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJUO9fDcPyAT"
      },
      "source": [
        "def read_indiana(file_name, cxr_path, mask_path):\n",
        "  \n",
        "  dcm_filename = file_name[3:-4] + '.dcm'\n",
        "\n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "\n",
        "  # Read in DICOM \n",
        "  ds = dcm.dcmread(os.path.join(cxr_path, dcm_filename))\n",
        "  cxr = ds.pixel_array\n",
        "  #print(ds.PhotometricInterpretation)\n",
        "  #cxr = dcm.pixel_data_handlers.util.apply_voi_lut(cxr, ds, index=0)\n",
        "  cxr = cv2.resize(cxr, (1024, 1024))\n",
        "  if ds.PhotometricInterpretation == 'MONOCHROME1':\n",
        "    cxr = 1.0 - cxr * 1./4096\n",
        "  else:\n",
        "    cxr = cxr * 1. / 4096\n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "  # cxr = exposure.equalize_hist(cxr)*255\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4vFwmwmqXGB"
      },
      "source": [
        "def read_nih(file_name, cxr_path, mask_path):\n",
        "  \n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "  \n",
        "  # Read in PNG\n",
        "  cxr = cv2.imread(os.path.join(cxr_path, file_name))\n",
        "  cxr = cv2.cvtColor(cxr, cv2.COLOR_BGR2GRAY)\n",
        "  cxr = cv2.resize(cxr, (1024, 1024)) \n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "  # cxr = exposure.equalize_hist(cxr)*255\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mydBwHhoDyqa"
      },
      "source": [
        "def read_jsrt(file_name, cxr_path, mask_path):\n",
        "\n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "\n",
        "  # Read in IMG\n",
        "  fname = file_name[:-4] + '.IMG'\n",
        "  cxr = 1.0 - np.fromfile(os.path.join(cxr_path, fname), dtype='>u2').reshape((2048, 2048))\n",
        "  cxr = cxr * 1. / 4096\n",
        "  cxr = cv2.resize(cxr, (1024, 1024)) \n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "  # cxr = exposure.equalize_hist(cxr)*255\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WueMguSeLvj9"
      },
      "source": [
        "# Patching Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCg6Qh1FRz4Q"
      },
      "source": [
        "def crop_patches(image, mask, total_patches, box, image_name, dataset, svpath):\n",
        "  if dataset == 'indiana':\n",
        "    patch_extractor = ExtractPatches(image_name, image, mask, box, 64, 32, dataset, svpath)\n",
        "  else:\n",
        "    patch_extractor = ExtractPatches(image_name, image, mask, box, 64, 16, dataset, svpath)\n",
        "  \n",
        "  lung_area = np.count_nonzero(mask)\n",
        "  patches = patch_extractor.extract_all_patches()\n",
        "  total_patches = total_patches + patches\n",
        "\n",
        "  return total_patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ASbq9ZR1sa"
      },
      "source": [
        "class ExtractPatches:\n",
        "    def __init__(self, image_name, image, mask, bbox, patchSize, stride, dataset, out_path):\n",
        "        self.image = image\n",
        "        self.out_path = out_path\n",
        "        self.dataset = dataset\n",
        "        self.bbox = bbox\n",
        "        self.annotated = np.copy(image) \n",
        "        self.reconstructed = np.empty(image.shape)\n",
        "        self.reconstructed[:] = np.nan\n",
        "        self.image_name = image_name\n",
        "        self.patchSize = patchSize\n",
        "        self.stride = stride\n",
        "        self.mask = mask\n",
        "\n",
        "    def draw_single_patches(self, coords, patch):\n",
        "      y1 = coords[0] * self.stride\n",
        "      x1 = coords[1] * self.stride\n",
        "      y2 = coords[0] * self.stride + self.patchSize\n",
        "      x2 = coords[1] * self.stride + self.patchSize\n",
        "      self.annotated = cv2.rectangle(self.annotated,(x1,y1), (x2,y2), (0, 0, 255))\n",
        "      self.reconstructed[y1:y2, x1:x2] = patch\n",
        "    \n",
        "    def extract_single_patches(self, patch):\n",
        "        croppedPatches = self.image[(patch[0] * self.stride):(patch[0] * self.stride + self.patchSize), \n",
        "                               (patch[1] * self.stride):(patch[1] * self.stride + self.patchSize)]\n",
        "        return croppedPatches\n",
        "\n",
        "    def no_of_patches(self):\n",
        "        yNoOfPatches, xNoOfPatches = (int((self.image.shape[1] - self.patchSize) / self.stride + 1),\n",
        "                                      int((self.image.shape[0] - self.patchSize) / self.stride + 1))\n",
        "        return xNoOfPatches, yNoOfPatches\n",
        "\n",
        "    def extract_all_patches(self):\n",
        "        xNoOfPatches, yNoOfPatches = self.no_of_patches()\n",
        "        closest = None\n",
        "        closest_dist = 10000\n",
        "\n",
        "        allPatches = list()\n",
        "        for y in range(yNoOfPatches):\n",
        "            for x in range(xNoOfPatches):\n",
        "              patch = self.extract_single_patches((x,y))\n",
        "                            \n",
        "              y1 = x * self.stride\n",
        "              x1 = y * self.stride\n",
        "              y2 = x * self.stride + self.patchSize\n",
        "              x2 = y * self.stride + self.patchSize\n",
        "              mask_cover = np.sum(self.mask[y1:y2, x1:x2])\n",
        "              if x1 > 0 and x2 < 1024 and y1 > 0 and y2 < 1024:\n",
        "                if patch.shape[0] == self.patchSize and patch.shape[1] and mask_cover/(255*self.patchSize*self.patchSize) > 0.75:\n",
        "                  if self.dataset == 'jsrt-nodule':\n",
        "                      cx = self.bbox[0]\n",
        "                      cy = self.bbox[1]\n",
        "                      if (x1 < cx) and (x2 > cx) and (y1 < cy) and (y2 > cy):\n",
        "                          bbox_cx = (x1+x2)//2\n",
        "                          bbox_cy = (y1+y2)//2\n",
        "                          if ((bbox_cx-cx)**2 + (bbox_cy-cy)**2) < closest_dist:\n",
        "                              closest_dist = (bbox_cx-cx)**2 + (bbox_cy-cy)**2\n",
        "                              closest = patch\n",
        "                              closest_x = x\n",
        "                              closest_y = y\n",
        "                          self.draw_single_patches((x,y), patch)\n",
        "                  elif self.dataset == 'nih-nodule':\n",
        "                      cx = (self.bbox[0] + self.bbox[2])//2\n",
        "                      cy = (self.bbox[1] + self.bbox[3])//2\n",
        "                      if (x1 < cx) and (x2 > cx) and (y1 < cy) and (y2 > cy):\n",
        "                          bbox_cx = (x1+x2)//2\n",
        "                          bbox_cy = (y1+y2)//2\n",
        "                          if ((bbox_cx-cx)**2 + (bbox_cy-cy)**2) < closest_dist:\n",
        "                              closest_dist = (bbox_cx-cx)**2 + (bbox_cy-cy)**2\n",
        "                              closest = patch\n",
        "                              closest_x = x\n",
        "                              closest_y = y\n",
        "                          self.draw_single_patches((x,y), patch)\n",
        "                  elif self.dataset == \"jsrt-healthy\":\n",
        "                      allPatches.append(patch)\n",
        "                      # cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                      # cv2.imwrite(\"data/test/healthy/\" + str(x) + \"_\" + str(y) + \"_\" + self.image_name, patch)\n",
        "                      plt.imsave(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      plt.imsave(\"data/test/healthy/\" + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      self.draw_single_patches((x,y), patch)\n",
        "                  elif self.dataset == \"jsrt-healthy-v\":\n",
        "                      allPatches.append(patch)\n",
        "                      # cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                      # cv2.imwrite(\"data/test/healthy/\" + str(x) + \"_\" + str(y) + \"_\" + self.image_name, patch)\n",
        "                      plt.imsave(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      plt.imsave(\"data/valid/healthy/\" + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      self.draw_single_patches((x,y), patch)\n",
        "                  else:\n",
        "                      allPatches.append(patch)\n",
        "                      # cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                      plt.imsave(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      self.draw_single_patches((x,y), patch)\n",
        "        \n",
        "        if closest is not None:\n",
        "            allPatches.append(closest)\n",
        "            # cv2.imwrite(self.out_path + str(closest_x) + '_' + str(closest_y) + '_' + self.image_name, closest)\n",
        "            plt.imsave(self.out_path + str(closest_x) + '_' + str(closest_y) + '_' + self.image_name, closest, cmap='gray')\n",
        "\n",
        "        #cv2_imshow(self.annotated)\n",
        "\n",
        "        return allPatches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srnZG0rGMFrG"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFY9EUIIR6jk"
      },
      "source": [
        "jsrt_mask_path = '/content/drive/MyDrive/FYDP 2021/JSRT Data/JSRT Masks'\n",
        "jsrt_cxr_path = '/content/drive/MyDrive/FYDP 2021/JSRT Data/All247images'\n",
        "\n",
        "nih_healthy_mask_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Healthy_masks'\n",
        "nih_healthy_cxr_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Healthy_raw'\n",
        "\n",
        "nih_nodule_mask_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Nodule_masks'\n",
        "nih_nodule_cxr_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Nodule_raw'\n",
        "\n",
        "indiana_mask_path = '/content/drive/My Drive/FYDP 2021/Indiana University Database/Additional Masks'\n",
        "indiana_cxr_path = '/content/drive/My Drive/FYDP 2021/Indiana University Database/DICOM Frontal'\n",
        "\n",
        "nih_healthy_img_list = os.listdir(nih_healthy_mask_path)\n",
        "indiana_img_list = os.listdir(indiana_mask_path)\n",
        "\n",
        "nih_info = pd.read_csv('/content/drive/MyDrive/FYDP 2021/NIH-Data/BBox_List_2017.csv')\n",
        "nih_info = nih_info[nih_info['Finding Label']=='Nodule']\n",
        "nih_nodule_list = nih_info['Image Index'].values\n",
        "\n",
        "jsrt_info = pd.read_csv('/content/drive/MyDrive/FYDP 2021/JSRT Data/jsrt_metadata (1).csv')\n",
        "jsrt_healthy_info = jsrt_info[jsrt_info['state']=='non-nodule']\n",
        "jsrt_healthy_list = jsrt_healthy_info['study_id'].values\n",
        "jsrt_nodule_info = jsrt_info[jsrt_info['state']!='non-nodule']\n",
        "jsrt_nodule_list = jsrt_nodule_info['study_id'].values\n",
        "\n",
        "batchsize = 64\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZEfvr2VNMNy"
      },
      "source": [
        "# Patients Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZncNrftK36C"
      },
      "source": [
        "# JSRT patients class\n",
        "class Patient ():\n",
        "  def __init__(self, ID, real_label, patients_path):\n",
        "      self.ID= ID\n",
        "      self.real_label = real_label\n",
        "      self.image_path = jsrt_cxr_path = '/content/drive/MyDrive/FYDP 2021/JSRT Data/All247images/' + ID[:-4] + '.IMG'\n",
        "      self.out_path = \"output/patients/\" + ID[:-4] + \"/\"\n",
        "      self.data_path = patients_path + ID[:-4] + \"/\"\n",
        "      self.patches = []\n",
        "      self.patches_list = []\n",
        "      self.fake_label = \"\"\n",
        "      self.anon_scores = []\n",
        "      self.anon_patches = []\n",
        "      self.anon_list = []\n",
        "      self.anon_scores_pos = []\n",
        "      self.batchsize = 64\n",
        "      self.isValid = False\n",
        "  \n",
        "  def make_folders(self):\n",
        "      os.mkdir(self.data_path)\n",
        "      os.mkdir(self.out_path)\n",
        "      os.mkdir(self.data_path + \"images/\")\n",
        "  \n",
        "  def draw_anon_patches(self):\n",
        "        cxr = 1.0 - np.fromfile(os.path.join(self.image_path), dtype='>u2').reshape((2048, 2048))*1./4096\n",
        "        cxr = cv2.resize(cxr, (1024, 1024))\n",
        "        normalizedImg = np.zeros((1024, 1024))\n",
        "        cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "        cxr = exposure.equalize_hist(cxr)*255\n",
        "\n",
        "        patch = self.get_real_nodule()\n",
        "        if patch:\n",
        "            x = patch.split(\"_\")[0]\n",
        "            y = patch.split(\"_\")[1]\n",
        "            x1 = int(x)*16\n",
        "            y1 = int(y)*16\n",
        "            x2 = int(x)*16 + 64\n",
        "            y2 = int(y)*16 + 64\n",
        "            #cxr = cv2.rectangle(cxr, (y1,x1),(y2,x2),(0,255,0),2) \n",
        "        for patch in self.anon_list:\n",
        "            x = patch.split(\"_\")[0]\n",
        "            y = patch.split(\"_\")[1]\n",
        "            x1 = int(x)*16\n",
        "            y1 = int(y)*16\n",
        "            x2 = int(x)*16 + 64\n",
        "            y2 = int(y)*16 + 64\n",
        "            cxr = cv2.rectangle(cxr, (y1,x1),(y2,x2),(0,0,255),1)\n",
        "        cv2.imwrite(self.out_path + \"anotated.png\", cxr)\n",
        "        return cxr\n",
        "  \n",
        "  def display_anon_image(self):\n",
        "      cv2_imshow(self.draw_anon_patches())\n",
        "  \n",
        "  def save_anon_patches(self):\n",
        "      i = 0\n",
        "      for patch in self.anon_patches:\n",
        "          vutils.save_image(patch,os.path.join(self.out_path,self.anon_list[i]), normalize=True)\n",
        "          i += 1\n",
        "  \n",
        "  def get_real_nodule(self):\n",
        "    if self.real_label == \"healthy\":\n",
        "        return False\n",
        "    else:\n",
        "        for nod in os.listdir(\"data/test/nodule/\"):\n",
        "          if nod [6:] == self.ID:\n",
        "              return nod\n",
        "    return False\n",
        "   \n",
        "  def dataloaders(self):\n",
        "      test_transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                              (0.5, 0.5, 0.5)),\n",
        "          transforms.Grayscale(num_output_channels=1),\n",
        "        ])\n",
        "\n",
        "      patient_dataset = datasets.ImageFolder(root=self.data_path, transform=test_transform)\n",
        "      return torch.utils.data.DataLoader(patient_dataset, batch_size=self.batchsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVMtuxzlL3Ov"
      },
      "source": [
        "def process_patients_results(patients):\n",
        "    true_positives = []\n",
        "    true_negatives = []\n",
        "    false_positives = []\n",
        "    false_negatives = []\n",
        "    for patient in patients:\n",
        "        if patient.real_label == \"healthy\":\n",
        "            if patient.fake_label == \"healthy\":\n",
        "                true_negatives.append(patient)\n",
        "            else:\n",
        "                false_positives.append(patient)\n",
        "        else:\n",
        "            if patient.fake_label == \"unhealthy\":\n",
        "                true_positives.append(patient)\n",
        "            else:\n",
        "                false_negatives.append(patient)\n",
        "    print(\"Number of True Positives: \", len(true_positives))\n",
        "    print(\"Number of True Negatives: \", len(true_negatives))\n",
        "    print(\"Number of False Positives: \", len(false_positives))\n",
        "    print(\"Number of False Negatives: \", len(false_negatives))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJ-gVUdQd4-"
      },
      "source": [
        "# Training Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEuy6dVytbdh"
      },
      "source": [
        "# training flags\n",
        "class TrainingOptions ():\n",
        "  def initiliaze(self):\n",
        "    self.gpu_ids = [0]\n",
        "    self.ngpu = 1\n",
        "    self.device = \"gpu\"\n",
        "    self.isTrain = True\n",
        "    self.train_data_path = \"data/train/\"\n",
        "    self.validation_data_path = \"data/valid/\"\n",
        "    self.test_data_path = \"data/test/\"\n",
        "    self.shuffle_data = True\n",
        "    self.dataset = \"nodule\"\n",
        "    self.split_jsrt = True # use JSRT as valid\n",
        "    self.SSIM = True # use SSIM as loss in testing\n",
        "    self.reconLoss = True # use recon loss in testing\n",
        "    self.batchsize = batchsize\n",
        "    self.isize = 64\n",
        "    self.nc = 1\n",
        "    self.nz = 100\n",
        "    self.ngf = 64\n",
        "    self.ndf = 64\n",
        "    self.extralayers = 0\n",
        "\n",
        "    self.name = \"test\"\n",
        "    self.model = \"ganomaly\"\n",
        "    self.display = False\n",
        "    self.outf = \"./output/\"\n",
        "\n",
        "    self.manualseed = 42\n",
        "    self.metric = 'roc'\n",
        "    self.print_freq = 100\n",
        "    self.save_image_freq = 100\n",
        "    self.save_test_images = True\n",
        "    self.resume = ''\n",
        "    self.phase = 'train'\n",
        "\n",
        "    self.iter = 1\n",
        "    self.niter = 50\n",
        "    self.niter_decay = 25\n",
        "    self.beta1 = 0.5\n",
        "    self.lr = 0.0002\n",
        "    self.w_adv = 1\n",
        "    self.w_con = 1\n",
        "    self.w_enc = 1\n",
        "    self.weight_decay = 1e-5\n",
        "    return self\n",
        "\n",
        "  def load_weights(self):\n",
        "    if self.isTrain: return False\n",
        "    else: return True\n",
        "\n",
        "    \n",
        "opt = TrainingOptions().initiliaze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFTkDdRVQgvp"
      },
      "source": [
        "# Read Images and Save Patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C5TG91-Qltf"
      },
      "source": [
        "### Indiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFJrH5R6SFFf"
      },
      "source": [
        "total_patches = []\n",
        "for j in range(len(indiana_img_list)):\n",
        "    image_name = indiana_img_list[j]\n",
        "    print(image_name)\n",
        "    box = []\n",
        "    mask, cxr = read_indiana(image_name, indiana_cxr_path, indiana_mask_path)\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'indiana', 'data/train/healthy/')\n",
        "len(total_patches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEMivvHQpR8"
      },
      "source": [
        "### NIH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XJxmrW_q0_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb0bb97-2e6a-457f-8ba4-5be4c8450512"
      },
      "source": [
        "if opt.split_jsrt:\n",
        "    print(\"NIH is not being used in this run...\")\n",
        "else:\n",
        "    total_patches = []\n",
        "    for j in range(len(nih_healthy_img_list)):\n",
        "        image_name = nih_healthy_img_list[j]\n",
        "        print(image_name)\n",
        "        box = []\n",
        "        mask, cxr = read_nih(image_name, nih_healthy_cxr_path, nih_healthy_mask_path)\n",
        "        total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'nih-healthy', 'data/valid/healthy/')\n",
        "    print(len(total_patches))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NIH is not being used in this run...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpgc3PgwS06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93a4913-d128-4cc5-c697-3c3f99072e00"
      },
      "source": [
        "if opt.split_jsrt:\n",
        "    print(\"NIH is not being used in this run...\")\n",
        "else:\n",
        "    total_patches = []\n",
        "    for j in range(len(nih_nodule_list)):\n",
        "        image_name = nih_nodule_list[j]\n",
        "        print(image_name)\n",
        "        bx = nih_info['Bbox [x'].values[j]\n",
        "        by = nih_info['y'].values[j]\n",
        "        bh = nih_info['h]'].values[j]\n",
        "        bw = nih_info['w'].values[j]\n",
        "        box = [bx,by, bx+bw, by+bh]\n",
        "        mask, cxr = read_nih(image_name, nih_nodule_cxr_path, nih_nodule_mask_path)\n",
        "        total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'nih-nodule', 'data/valid/nodule/')\n",
        "    print(len(total_patches))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NIH is not being used in this run...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RREgCaNsQ5j"
      },
      "source": [
        "if not opt.split_jsrt :\n",
        "    files = os.listdir(path='data/valid/healthy')\n",
        "    nfiles = len(files) + len(os.listdir(\"data/valid/nodule\"))\n",
        "    if nfiles % batchsize != 0:\n",
        "      del_files = random.sample(files,nfiles%batchsize)\n",
        "      for dfile in del_files:\n",
        "        os.remove(os.path.join(\"data/valid/healthy/\", dfile))\n",
        "      print(\"Files removed: \", len(del_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4jg1DKwQx-u"
      },
      "source": [
        "### JSRT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MgQLGMXEsnh"
      },
      "source": [
        "total_patches = []\n",
        "for j in range(len(jsrt_nodule_list)):\n",
        "    image_name = jsrt_nodule_list[j]\n",
        "    print(image_name)\n",
        "    bx = jsrt_nodule_info['x'].values[j]/2\n",
        "    by = jsrt_nodule_info['y'].values[j]/2\n",
        "    box = [bx,by]\n",
        "    mask, cxr = read_jsrt(image_name, jsrt_cxr_path, jsrt_mask_path)\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'jsrt-nodule', 'data/test/nodule/')\n",
        "\n",
        "if opt.split_jsrt:\n",
        "    print(\"Splitting nodules in half for validation and test...\")\n",
        "    files = os.listdir(path=\"data/test/nodule/\")\n",
        "    move_files = random.sample(files,75)\n",
        "    for mfile in move_files:\n",
        "        os.rename(\"data/test/nodule/\" + mfile, \"data/valid/nodule/\" + mfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUKmAgthL5Tv",
        "outputId": "b2d40f66-0c91-4ff8-9b9a-79ed0f80913a"
      },
      "source": [
        "# create JSRT patients\n",
        "patients = []\n",
        "patients_path = \"data/patients/\"\n",
        "# adding healthy patients\n",
        "for id in jsrt_info['study_id']:\n",
        "  if id[:5] == \"JPCNN\":\n",
        "      patients.append(Patient(id,\"healthy\", patients_path))\n",
        "\n",
        "# making patients for validation\n",
        "if opt.split_jsrt:\n",
        "    for patient in random.sample(patients,10):\n",
        "        patient.isValid = True\n",
        "\n",
        "# adding unhealthy patients\n",
        "for id in jsrt_info['study_id']:\n",
        "  if id[:5] == \"JPCLN\":\n",
        "      patients.append(Patient(id, \"unhealthy\", patients_path))\n",
        "\n",
        "print(len(patients), jsrt_info.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "246 (246, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Th0W1WMCqz"
      },
      "source": [
        "# create and save patches per patient (also saves the healthy ones under data/test/healthy)\n",
        "niter = 0\n",
        "test_patients = [patients[0], patients[196]]\n",
        "for patient in patients:\n",
        "    patient.make_folders()\n",
        "    mask, cxr = read_jsrt(patient.ID, jsrt_cxr_path, jsrt_mask_path)\n",
        "    if patient.isValid:\n",
        "        print(\"Patient %s used for valid\" % (patient.ID))\n",
        "        patient.patches = crop_patches(cxr, mask, [], [], patient.ID, 'jsrt-' + patient.real_label + \"-v\", patient.data_path + \"images/\")\n",
        "        niter += 1\n",
        "    else:\n",
        "        print(patient.ID)\n",
        "        patient.patches = crop_patches(cxr, mask, [], [], patient.ID, 'jsrt-' + patient.real_label, patient.data_path + \"images/\")\n",
        "    patient.patches_list = os.listdir(patient.data_path + \"images/\")\n",
        "    patient.patches_list.sort()\n",
        "  \n",
        "  print(\"Number of patients used for validation: \", niter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS3NrV9uKxXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91aa942e-434e-48f4-a612-f9c4eb618a44"
      },
      "source": [
        "files = os.listdir(path='data/test/healthy')\n",
        "nfiles = len(files) + len(os.listdir(\"data/test/nodule\"))\n",
        "if nfiles % batchsize != 0:\n",
        "  del_files = random.sample(files,nfiles%batchsize)\n",
        "  for dfile in del_files:\n",
        "    os.remove(os.path.join(\"data/test/healthy/\", dfile))\n",
        "  print(\"Files removed: \", len(del_files))\n",
        "\n",
        "if opt.split_jsrt:\n",
        "  files = os.listdir(path='data/valid/healthy')\n",
        "  nfiles = len(files) + len(os.listdir(\"data/valid/nodule\"))\n",
        "  if nfiles % batchsize != 0:\n",
        "    del_files = random.sample(files,nfiles%batchsize)\n",
        "    for dfile in del_files:\n",
        "        os.remove(os.path.join(\"data/valid/healthy/\", dfile))\n",
        "  print(\"Files removed: \", len(del_files))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files removed:  11\n",
            "Files removed:  62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqtJPL6Ptg9q"
      },
      "source": [
        "# Model Code: Helper Functions and NNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph0dSBActujt"
      },
      "source": [
        "def get_dataloaders(train_data_path, test_data_path, bs=64):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(5, translate=[0.05,0.05]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "    ])\n",
        "    \n",
        "    trn_dataset = datasets.ImageFolder(root=train_data_path, transform=train_transform)\n",
        "    tst_dataset = datasets.ImageFolder(root=test_data_path, transform=test_transform)\n",
        "\n",
        "    trn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=bs, shuffle=True, drop_last=True)\n",
        "    tst_loader = torch.utils.data.DataLoader(tst_dataset, batch_size=bs)\n",
        "  \n",
        "    return trn_loader, tst_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKCZ8rVZt5BQ"
      },
      "source": [
        "def weights_init(mod):\n",
        "\n",
        "    classname = mod.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        mod.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        mod.weight.data.normal_(1.0, 0.02)\n",
        "        mod.bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DCm6AxMt61g"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0, add_final_conv=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
        "\n",
        "        main = nn.Sequential()\n",
        "        # input is nc x isize x isize\n",
        "        main.add_module('initial-conv-{0}-{1}'.format(nc, ndf),\n",
        "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
        "        main.add_module('initial-relu-{0}'.format(ndf),\n",
        "                        nn.LeakyReLU(0.2, inplace=True))\n",
        "        csize, cndf = isize / 2, ndf\n",
        "\n",
        "        # Extra layers\n",
        "        for t in range(n_extra_layers):\n",
        "            main.add_module('extra-layers-{0}-{1}-conv'.format(t, cndf),\n",
        "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
        "            main.add_module('extra-layers-{0}-{1}-batchnorm'.format(t, cndf),\n",
        "                            nn.BatchNorm2d(cndf))\n",
        "            main.add_module('extra-layers-{0}-{1}-relu'.format(t, cndf),\n",
        "                            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        while csize > 4:\n",
        "            in_feat = cndf\n",
        "            out_feat = cndf * 2\n",
        "            main.add_module('pyramid-{0}-{1}-conv'.format(in_feat, out_feat),\n",
        "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
        "            main.add_module('pyramid-{0}-batchnorm'.format(out_feat),\n",
        "                            nn.BatchNorm2d(out_feat))\n",
        "            main.add_module('pyramid-{0}-relu'.format(out_feat),\n",
        "                            nn.LeakyReLU(0.2, inplace=True))\n",
        "            cndf = cndf * 2\n",
        "            csize = csize / 2\n",
        "\n",
        "        # state size. K x 4 x 4\n",
        "        if add_final_conv:\n",
        "            main.add_module('final-{0}-{1}-conv'.format(cndf, 1),\n",
        "                            nn.Conv2d(cndf, nz, 4, 1, 0, bias=False))\n",
        "\n",
        "        self.main = main\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4awyPW6t9-Y"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, isize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
        "\n",
        "        cngf, tisize = ngf // 2, 4\n",
        "        while tisize != isize:\n",
        "            cngf = cngf * 2\n",
        "            tisize = tisize * 2\n",
        "\n",
        "        main = nn.Sequential()\n",
        "        # input is Z, going into a convolution\n",
        "        main.add_module('initial-{0}-{1}-convt'.format(nz, cngf),\n",
        "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
        "        main.add_module('initial-{0}-batchnorm'.format(cngf),\n",
        "                        nn.BatchNorm2d(cngf))\n",
        "        main.add_module('initial-{0}-relu'.format(cngf),\n",
        "                        nn.ReLU(True))\n",
        "\n",
        "        csize, _ = 4, cngf\n",
        "        while csize < isize // 2:\n",
        "            main.add_module('pyramid-{0}-{1}-convt'.format(cngf, cngf // 2),\n",
        "                            nn.ConvTranspose2d(cngf, cngf // 2, 4, 2, 1, bias=False))\n",
        "            main.add_module('pyramid-{0}-batchnorm'.format(cngf // 2),\n",
        "                            nn.BatchNorm2d(cngf // 2))\n",
        "            main.add_module('pyramid-{0}-relu'.format(cngf // 2),\n",
        "                            nn.ReLU(True))\n",
        "            cngf = cngf // 2\n",
        "            csize = csize * 2\n",
        "\n",
        "        # Extra layers\n",
        "        for t in range(n_extra_layers):\n",
        "            main.add_module('extra-layers-{0}-{1}-conv'.format(t, cngf),\n",
        "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
        "            main.add_module('extra-layers-{0}-{1}-batchnorm'.format(t, cngf),\n",
        "                            nn.BatchNorm2d(cngf))\n",
        "            main.add_module('extra-layers-{0}-{1}-relu'.format(t, cngf),\n",
        "                            nn.ReLU(True))\n",
        "\n",
        "        main.add_module('final-{0}-{1}-convt'.format(cngf, nc),\n",
        "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
        "        main.add_module('final-{0}-tanh'.format(nc),\n",
        "                        nn.Tanh())\n",
        "        self.main = main\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48OKZqR1uAcg"
      },
      "source": [
        "class NetD(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NetD, self).__init__()\n",
        "        model = Encoder(opt.isize, 1, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n",
        "        layers = list(model.main.children())\n",
        "\n",
        "        self.features = nn.Sequential(*layers[:-1])\n",
        "        self.classifier = nn.Sequential(layers[-1])\n",
        "        self.classifier.add_module('Sigmoid', nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        features = features\n",
        "        classifier = self.classifier(features)\n",
        "        classifier = classifier.view(-1, 1).squeeze(1)\n",
        "\n",
        "        return classifier, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OIJajGEuCSr"
      },
      "source": [
        "class NetG(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NetG, self).__init__()\n",
        "        self.encoder1 = Encoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n",
        "        self.decoder = Decoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n",
        "        self.encoder2 = Encoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_i = self.encoder1(x)\n",
        "        gen_imag = self.decoder(latent_i)\n",
        "        latent_o = self.encoder2(gen_imag)\n",
        "        return gen_imag, latent_i, latent_o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae39PnpKuCwQ"
      },
      "source": [
        "from pytorch_msssim import ssim\n",
        "def l1_loss(input, target):\n",
        "    return torch.mean(torch.abs(input - target))\n",
        "\n",
        "def l2_loss(input, target, size_average=True):\n",
        "    if size_average:\n",
        "        return torch.mean(torch.pow((input-target), 2))\n",
        "    else:\n",
        "        return torch.pow((input-target), 2)\n",
        "\n",
        "def ssim_loss(input,target,size_average=True):\n",
        "    input = (input + 1) / 2\n",
        "    target = (target + 1) / 2\n",
        "    ssim_loss = 1 - ssim(input, target, data_range=1, size_average=size_average)\n",
        "    return ssim_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYiSmn_zuJqL"
      },
      "source": [
        "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
        "rc('text', usetex=True)\n",
        "\n",
        "def evaluate(labels, scores, metric='roc'):\n",
        "    if metric == 'roc':\n",
        "        return roc(labels, scores)\n",
        "    elif metric == 'auprc':\n",
        "        return auprc(labels, scores)\n",
        "    elif metric == 'f1_score':\n",
        "        threshold = 0.20\n",
        "        scores[scores >= threshold] = 1\n",
        "        scores[scores <  threshold] = 0\n",
        "        return f1_score(labels.cpu(), scores.cpu())\n",
        "    else:\n",
        "        raise NotImplementedError(\"Check the evaluation metric.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj2TM-gR1Pua"
      },
      "source": [
        "def roc(labels, scores, saveto=None):\n",
        "    \"\"\"Compute ROC curve and ROC area for each class\"\"\"\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    scores = scores.cpu()\n",
        "\n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, _ = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Equal Error Rate\n",
        "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "    return roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0pGSq32uMCN"
      },
      "source": [
        "def auprc(labels, scores):\n",
        "    ap = average_precision_score(labels.cpu(), scores.cpu())\n",
        "    return ap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8-3sLkwuSfl"
      },
      "source": [
        "class Visualizer():\n",
        "    # pylint: disable=too-many-instance-attributes\n",
        "    # Reasonable.\n",
        "\n",
        "    def __init__(self):\n",
        "        self.win_size = 256\n",
        "        self.name = opt.name\n",
        "        if opt.display:\n",
        "            import visdom\n",
        "\n",
        "        # --\n",
        "        # Dictionaries for plotting data and results.\n",
        "        self.plot_data = None\n",
        "        self.plot_res = None\n",
        "\n",
        "        # --\n",
        "        # Path to train and test directories.\n",
        "        self.img_dir = os.path.join(opt.outf, opt.name, 'train', 'images')\n",
        "        self.tst_img_dir = os.path.join(opt.outf, opt.name, 'test', 'images')\n",
        "        if not os.path.exists(self.img_dir):\n",
        "            os.makedirs(self.img_dir)\n",
        "        if not os.path.exists(self.tst_img_dir):\n",
        "            os.makedirs(self.tst_img_dir)\n",
        "        # --\n",
        "        # Log file.\n",
        "        self.log_name = os.path.join(opt.outf, opt.name, 'loss_log.txt')\n",
        "        with open(self.log_name, \"a\") as log_file:\n",
        "            now = time.strftime(\"%c\")\n",
        "            log_file.write('================ Training Loss (%s) ================\\n' % now)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(inp):\n",
        "        return (inp - inp.min()) / (inp.max() - inp.min() + 1e-5)\n",
        "\n",
        "    def plot_current_errors(self, epoch, counter_ratio, errors):\n",
        "\n",
        "        if not hasattr(self, 'plot_data') or self.plot_data is None:\n",
        "            self.plot_data = {'X': [], 'Y': [], 'legend': list(errors.keys())}\n",
        "        self.plot_data['X'].append(epoch + counter_ratio)\n",
        "        self.plot_data['Y'].append([errors[k] for k in self.plot_data['legend']])\n",
        "        self.vis.line(\n",
        "            X=np.stack([np.array(self.plot_data['X'])] * len(self.plot_data['legend']), 1),\n",
        "            Y=np.array(self.plot_data['Y']),\n",
        "            opts={\n",
        "                'title': self.name + ' loss over time',\n",
        "                'legend': self.plot_data['legend'],\n",
        "                'xlabel': 'Epoch',\n",
        "                'ylabel': 'Loss'\n",
        "            },\n",
        "            win=4\n",
        "        )\n",
        "\n",
        "    def plot_performance(self, epoch, counter_ratio, performance):\n",
        "\n",
        "        if not hasattr(self, 'plot_res') or self.plot_res is None:\n",
        "            self.plot_res = {'X': [], 'Y': [], 'legend': list(performance.keys())}\n",
        "        self.plot_res['X'].append(epoch + counter_ratio)\n",
        "        self.plot_res['Y'].append([performance[k] for k in self.plot_res['legend']])\n",
        "        self.vis.line(\n",
        "            X=np.stack([np.array(self.plot_res['X'])] * len(self.plot_res['legend']), 1),\n",
        "            Y=np.array(self.plot_res['Y']),\n",
        "            opts={\n",
        "                'title': self.name + 'Performance Metrics',\n",
        "                'legend': self.plot_res['legend'],\n",
        "                'xlabel': 'Epoch',\n",
        "                'ylabel': 'Stats'\n",
        "            },\n",
        "            win=5\n",
        "        )\n",
        "\n",
        "    def print_current_errors(self, epoch, errors):\n",
        "\n",
        "        message = '   Loss: [%d/%d] ' % (epoch, niter)\n",
        "        for key, val in errors.items():\n",
        "            message += '%s: %.3f ' % (key, val)\n",
        "\n",
        "        print(message)\n",
        "        with open(self.log_name, \"a\") as log_file:\n",
        "            log_file.write('%s\\n' % message)\n",
        "\n",
        "    def print_current_performance(self, performance, best):\n",
        "\n",
        "        message = '\\n   '\n",
        "        for key, val in performance.items():\n",
        "            message += '%s: %.3f ' % (key, val)\n",
        "        message += 'max ' + opt.metric + ': %.3f' % best\n",
        "\n",
        "        print(message)\n",
        "        with open(self.log_name, \"a\") as log_file:\n",
        "            log_file.write('%s\\n' % message)\n",
        "\n",
        "    def display_current_images(self, reals, fakes, fixed):\n",
        "\n",
        "        reals = self.normalize(reals.cpu().numpy())\n",
        "        fakes = self.normalize(fakes.cpu().numpy())\n",
        "        fixed = self.normalize(fixed.cpu().numpy())\n",
        "\n",
        "        self.vis.images(reals, win=1, opts={'title': 'Reals'})\n",
        "        self.vis.images(fakes, win=2, opts={'title': 'Fakes'})\n",
        "        self.vis.images(fixed, win=3, opts={'title': 'Fixed'})\n",
        "\n",
        "    def save_current_images(self, epoch, reals, fakes, fixed):\n",
        "\n",
        "        vutils.save_image(reals, '%s/reals.png' % self.img_dir, normalize=True)\n",
        "        vutils.save_image(fakes, '%s/fakes.png' % self.img_dir, normalize=True)\n",
        "        vutils.save_image(fixed, '%s/fixed_fakes_%03d.png' %(self.img_dir, epoch), normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0FHpA4RacK"
      },
      "source": [
        "# Model Code: Ganomaly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nqb1nKhuZzZ"
      },
      "source": [
        "class BaseModel():\n",
        "\n",
        "    def __init__(self, train_dataloader, test_dataloader):\n",
        "        self.seed(opt.manualseed)\n",
        "\n",
        "        # Initalize variables.\n",
        "        self.visualizer = Visualizer()\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.trn_dir = os.path.join(opt.outf, opt.name, 'train')\n",
        "        self.tst_dir = os.path.join(opt.outf, opt.name, 'test')\n",
        "        self.device = torch.device(\"cuda:0\" if opt.device != 'cpu' else \"cpu\")\n",
        "\n",
        "    def set_input(self, input:torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "            self.input.resize_(input[0].size()).copy_(input[0])\n",
        "            self.gt.resize_(input[1].size()).copy_(input[1])\n",
        "            self.label.resize_(input[1].size())\n",
        "\n",
        "            # Copy the first batch as the fixed input.\n",
        "            if self.total_steps == opt.batchsize:\n",
        "                self.fixed_input.resize_(input[0].size()).copy_(input[0])\n",
        "\n",
        "    def seed(self, seed_value):\n",
        "\n",
        "        # Check if seed is default value\n",
        "        if seed_value == -1:\n",
        "            return\n",
        "\n",
        "        # Otherwise seed all functionality\n",
        "        import random\n",
        "        random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.set_deterministic(True)\n",
        "\n",
        "    def get_errors(self):\n",
        "\n",
        "        errors = OrderedDict([\n",
        "            ('err_d', self.err_d.item()),\n",
        "            ('err_g', self.err_g.item()),\n",
        "            ('err_g_adv', self.err_g_adv.item()),\n",
        "            ('err_g_con', self.err_g_con.item()),\n",
        "            ('err_g_enc', self.err_g_enc.item())])\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def get_current_images(self):\n",
        "\n",
        "        reals = self.input.data\n",
        "        fakes = self.fake.data\n",
        "        fixed = self.netg(self.fixed_input)[0].data\n",
        "\n",
        "        return reals, fakes, fixed\n",
        "\n",
        "    def save_weights(self, epoch):\n",
        "\n",
        "        weight_dir = os.path.join(opt.outf, opt.name, 'train', 'weights')\n",
        "        if not os.path.exists(weight_dir): os.makedirs(weight_dir)\n",
        "\n",
        "        torch.save({'epoch': epoch + 1, 'state_dict': self.netg.state_dict()},\n",
        "                   '%s/netG.pth' % (weight_dir))\n",
        "        torch.save({'epoch': epoch + 1, 'state_dict': self.netd.state_dict()},\n",
        "                   '%s/netD.pth' % (weight_dir))\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        opt.phase = 'train'\n",
        "        self.netg.train()\n",
        "        epoch_iter = 0\n",
        "        for data in tqdm(self.train_dataloader, leave=False, total=len(self.train_dataloader)):\n",
        "            self.total_steps += opt.batchsize\n",
        "            epoch_iter += opt.batchsize\n",
        "            self.set_input(data)\n",
        "            # self.optimize()\n",
        "            self.optimize_params()\n",
        "\n",
        "            if self.total_steps % opt.print_freq == 0:\n",
        "                errors = self.get_errors()\n",
        "                if opt.display:\n",
        "                    counter_ratio = float(epoch_iter) / len(self.train_dataloader.dataset)\n",
        "                    self.visualizer.plot_current_errors(self.epoch, counter_ratio, errors)\n",
        "\n",
        "            if self.total_steps % opt.save_image_freq == 0:\n",
        "                reals, fakes, fixed = self.get_current_images()\n",
        "                self.visualizer.save_current_images(self.epoch, reals, fakes, fixed)\n",
        "                if opt.display:\n",
        "                    self.visualizer.display_current_images(reals, fakes, fixed)\n",
        "        self.update_learning_rate()\n",
        "        print(\"\\n>> Training model %s. Epoch %d/%d\" % (self.name, self.epoch, opt.niter + opt.niter_decay))\n",
        "\n",
        "    def train(self):\n",
        "        self.total_steps = 0\n",
        "        best_auc = 0\n",
        "\n",
        "        # Train for niter epochs.\n",
        "        print(\"\\n>> Training model %s.\" % self.name)\n",
        "        for self.epoch in range(opt.iter, opt.niter + opt.niter_decay + 1):\n",
        "            self.train_one_epoch()\n",
        "            res = self.test()\n",
        "            if res[opt.metric] > best_auc:\n",
        "                best_auc = res[opt.metric]\n",
        "                self.save_weights(self.epoch)\n",
        "            self.visualizer.print_current_performance(res, best_auc)\n",
        "        print(\"\\n>> Training model %s.[Done]\" % self.name)\n",
        "\n",
        "    def test(self):\n",
        "        with torch.no_grad():\n",
        "            if opt.load_weights():\n",
        "                path = \"./output/test/train/weights/netG.pth\"\n",
        "                pretrained_dict = torch.load(path)['state_dict']\n",
        "\n",
        "                try:\n",
        "                    self.netg.load_state_dict(pretrained_dict)\n",
        "                except IOError:\n",
        "                    raise IOError(\"netG weights not found\")\n",
        "                print('   Loaded weights.')\n",
        "\n",
        "            opt.phase = 'test'\n",
        "\n",
        "            # Create big error tensor for the test set\n",
        "            self.an_scores = torch.zeros(size=(len(self.test_dataloader.dataset),), dtype=torch.float32, device=self.device)\n",
        "            self.recon_scores = torch.zeros(size=(len(self.test_dataloader.dataset),), dtype=torch.float32, device=self.device)\n",
        "            self.gt_labels = torch.zeros(size=(len(self.test_dataloader.dataset),), dtype=torch.long,    device=self.device)\n",
        "            self.latent_i  = torch.zeros(size=(len(self.test_dataloader.dataset), opt.nz), dtype=torch.float32, device=self.device)\n",
        "            self.latent_o  = torch.zeros(size=(len(self.test_dataloader.dataset), opt.nz), dtype=torch.float32, device=self.device)\n",
        "\n",
        "            self.times = []\n",
        "            self.total_steps = 0\n",
        "            epoch_iter = 0\n",
        "            for i, data in enumerate(self.test_dataloader, 0):\n",
        "                self.total_steps += opt.batchsize\n",
        "                epoch_iter += opt.batchsize\n",
        "                time_i = time.time()\n",
        "                self.set_input(data)\n",
        "                self.fake, latent_i, latent_o = self.netg(self.input)\n",
        "\n",
        "                error = torch.mean(torch.pow((latent_i-latent_o), 2), dim=1)\n",
        "                if opt.SSIM:\n",
        "                    error2 = ssim_loss(self.fake,self.input, size_average=False)\n",
        "                else:\n",
        "                    rec_image = torch.squeeze(self.fake)\n",
        "                    in_image = torch.squeeze(self.input)\n",
        "                    error2 = torch.mean(torch.abs(rec_image.view(opt.batchsize, -1) - in_image.view(opt.batchsize, -1)), dim=1)\n",
        "    \n",
        "                time_o = time.time()\n",
        "                batchsize = opt.batchsize\n",
        "                self.an_scores[i*batchsize : i*batchsize+error.size(0)] = error.reshape(error.size(0))\n",
        "                self.recon_scores[i*batchsize : i*batchsize+error2.size(0)] = error2.reshape(error2.size(0))\n",
        "                self.gt_labels[i*batchsize : i*batchsize+error.size(0)] = self.gt.reshape(error.size(0))\n",
        "                self.latent_i [i*batchsize : i*batchsize+error.size(0), :] = latent_i.reshape(error.size(0), opt.nz)\n",
        "                self.latent_o [i*batchsize : i*batchsize+error.size(0), :] = latent_o.reshape(error.size(0), opt.nz)\n",
        "\n",
        "                self.times.append(time_o - time_i)\n",
        "\n",
        "                # Save test images.\n",
        "                if opt.save_test_images:\n",
        "                    dst = os.path.join(opt.outf, opt.name, 'test', 'images')\n",
        "                    if not os.path.isdir(dst):\n",
        "                        os.makedirs(dst)\n",
        "                    real, fake, _ = self.get_current_images()\n",
        "                    vutils.save_image(real, '%s/real_%03d.png' % (dst, i+1), normalize=True)\n",
        "                    vutils.save_image(fake, '%s/fake_%03d.png' % (dst, i+1), normalize=True)\n",
        "\n",
        "            # Measure inference time.\n",
        "            self.times = np.array(self.times)\n",
        "            self.times = np.mean(self.times[:100] * 1000)\n",
        "            #print(self.latent_i.shape)\n",
        "            emb_i_df = pd.DataFrame(self.latent_i.cpu().numpy())\n",
        "            emb_i_df.to_csv('tmp.csv')\n",
        "\n",
        "            emb_o_df = pd.DataFrame(self.latent_o.cpu().numpy())\n",
        "            emb_o_df.to_csv('tmp2.csv')\n",
        "\n",
        "            # Scale error vector between [0, 1]\n",
        "            self.an_scores = (self.an_scores - torch.min(self.an_scores)) / (torch.max(self.an_scores) - torch.min(self.an_scores))\n",
        "            self.recon_scores = (self.recon_scores - torch.min(self.recon_scores)) / (torch.max(self.recon_scores) - torch.min(self.recon_scores))\n",
        "\n",
        "            if opt.reconLoss:\n",
        "                self.an_scores = self.an_scores + self.recon_scores\n",
        "\n",
        "            auc = evaluate(self.gt_labels, self.an_scores, metric=opt.metric)\n",
        "            performance = OrderedDict([('Avg Run Time (ms/batch)', self.times), (opt.metric, auc)])\n",
        "\n",
        "            return performance\n",
        "\n",
        "    def test_patients(self, patients, threshold):\n",
        "        with torch.no_grad():\n",
        "            if opt.load_weights():\n",
        "                path = \"./output/test/train/weights/netG.pth\"\n",
        "                pretrained_dict = torch.load(path)['state_dict']\n",
        "\n",
        "                try:\n",
        "                    self.netg.load_state_dict(pretrained_dict)\n",
        "                except IOError:\n",
        "                    raise IOError(\"netG weights not found\")\n",
        "                print('   Loaded weights.')\n",
        "\n",
        "            opt.phase = 'test-patients'\n",
        "            self.total_steps = 0\n",
        "\n",
        "            for patient in patients:\n",
        "                print(\"Testing patient: \", patient.ID[:-4])\n",
        "                # reset list so we can't run it several times\n",
        "                patient.anon_list = []\n",
        "                patient.anon_scores = []\n",
        "                anon_scores = torch.zeros(size=(len(patient.dataloaders().dataset),), dtype=torch.float32, device=self.device)\n",
        "                recon_scores = torch.zeros(size=(len(patient.dataloaders().dataset),), dtype=torch.float32, device=self.device)\n",
        "                patient.anon_patches = []\n",
        "                patient.anon_scores_pos = []\n",
        "                for i, data in enumerate(patient.dataloaders(),0):\n",
        "                    self.set_input(data)\n",
        "                    self.fake, latent_i, latent_o = self.netg(self.input)\n",
        "                    error = torch.mean(torch.pow((latent_i - latent_o), 2), dim=1)\n",
        "                    rec_image = torch.squeeze(self.fake)\n",
        "                    in_image = torch.squeeze(self.input)\n",
        "                    error2 = torch.mean(torch.abs(rec_image.view(len(data[0]), -1) - in_image.view(len(data[0]), -1)), dim =1)\n",
        "                    print(len(data[0]),error.shape,error2.shape)\n",
        "                    anon_scores[i*patient.batchsize: i*patient.batchsize+error.size(0)] = error.reshape(error.size(0))\n",
        "                    recon_scores[i*patient.batchsize: i*patient.batchsize+error2.size(0)] = error2.reshape(error2.size(0))\n",
        "\n",
        "                anon_scores = (anon_scores - torch.min(anon_scores)) / (torch.max(anon_scores) - torch.min(anon_scores))\n",
        "                recon_scores = (recon_scores - torch.min(recon_scores)) / (torch.max(recon_scores) - torch.min(recon_scores))\n",
        "                anon_scores = anon_scores + recon_scores\n",
        "                patient.anon_scores = anon_scores.cpu()\n",
        "\n",
        "                for i in range(len(patient.anon_scores)):\n",
        "                    an_score = patient.anon_scores[i]\n",
        "                    if an_score > threshold[0] and an_score < threshold[1]:\n",
        "                        patient.anon_list.append(patient.patches_list[i])\n",
        "                        patient.anon_scores_pos.append(an_score)\n",
        "                \n",
        "                if len(patient.anon_scores_pos) == 0:\n",
        "                    patient.fake_label = \"healthy\"\n",
        "                else: patient.fake_label = \"unhealthy\"\n",
        "            \n",
        "            print(\"Patient testing finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pttVcfhdubh_"
      },
      "source": [
        "class Ganomaly(BaseModel):\n",
        "\n",
        "    @property\n",
        "    def name(self): return 'Ganomaly'\n",
        "\n",
        "    def __init__(self, train_dataloader, test_dataloader):\n",
        "        super(Ganomaly, self).__init__(train_dataloader, test_dataloader)\n",
        "\n",
        "        # -- Misc attributes\n",
        "        self.epoch = 0\n",
        "        self.times = []\n",
        "        self.total_steps = 0\n",
        "\n",
        "        # Create and initialize networks.\n",
        "        self.netg = NetG().to(self.device)\n",
        "        self.netd = NetD().to(self.device)\n",
        "        self.netg.apply(weights_init)\n",
        "        self.netd.apply(weights_init)\n",
        "\n",
        "        if opt.resume != '':\n",
        "            # TODO: save and load optimizers -- NB\n",
        "            print(\"\\nLoading pre-trained networks.\")\n",
        "            iter = torch.load(os.path.join(opt.resume, 'netG.pth'))['epoch']\n",
        "            self.netg.load_state_dict(torch.load(os.path.join(opt.resume, 'netG.pth'))['state_dict'])\n",
        "            self.netd.load_state_dict(torch.load(os.path.join(opt.resume, 'netD.pth'))['state_dict'])\n",
        "            print(\"\\tDone.\\n\")\n",
        "\n",
        "        self.l_adv = l2_loss\n",
        "        # self.l_con = nn.L1Loss()\n",
        "        self.l_con = ssim_loss\n",
        "        self.l_enc = l2_loss\n",
        "        self.l_bce = nn.BCELoss()\n",
        "\n",
        "        # Initialize input tensors.\n",
        "        batchsize = opt.batchsize\n",
        "        self.input = torch.empty(size=(batchsize, opt.nc, opt.isize, opt.isize), dtype=torch.float32, device=self.device)\n",
        "        self.label = torch.empty(size=(batchsize,), dtype=torch.float32, device=self.device)\n",
        "        self.gt    = torch.empty(size=(batchsize,), dtype=torch.long, device=self.device)\n",
        "        self.fixed_input = torch.empty(size=(batchsize, opt.nc, opt.isize, opt.isize), dtype=torch.float32, device=self.device)\n",
        "        self.real_label = torch.ones(size=(batchsize,), dtype=torch.float32, device=self.device)\n",
        "        self.fake_label = torch.zeros(size=(batchsize,), dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Setup optimizer\n",
        "        if opt.isTrain:\n",
        "            self.netg.train()\n",
        "            self.netd.train()\n",
        "            self.optimizer_d = optim.Adam(self.netd.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            self.optimizer_g = optim.Adam(self.netg.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            def lambda_rule(epoch):\n",
        "                lr_l = 1.0 - max(0, epoch + opt.iter - opt.niter) / float(opt.niter_decay + 1)\n",
        "                return lr_l\n",
        "            self.scheduler_d = optim.lr_scheduler.LambdaLR(self.optimizer_d, lr_lambda=lambda_rule)\n",
        "            self.scheduler_g = optim.lr_scheduler.LambdaLR(self.optimizer_g, lr_lambda=lambda_rule)\n",
        "\n",
        "    def forward_g(self):\n",
        "        self.fake, self.latent_i, self.latent_o = self.netg(self.input)\n",
        "\n",
        "    def forward_d(self):\n",
        "        self.pred_real, self.feat_real = self.netd(self.input)\n",
        "        self.pred_fake, self.feat_fake = self.netd(self.fake.detach())\n",
        "\n",
        "    def backward_g(self):\n",
        "        self.err_g_adv = self.l_adv(self.netd(self.input)[1], self.netd(self.fake)[1])\n",
        "        self.err_g_con = self.l_con(self.fake, self.input)\n",
        "        self.err_g_enc = self.l_enc(self.latent_o, self.latent_i)\n",
        "        self.err_g = self.err_g_adv * opt.w_adv + \\\n",
        "                     self.err_g_con * opt.w_con + \\\n",
        "                     self.err_g_enc * opt.w_enc\n",
        "        self.err_g.backward(retain_graph=True)\n",
        "\n",
        "    def backward_d(self):\n",
        "        # Real - Fake Loss\n",
        "        self.err_d_real = self.l_bce(self.pred_real, self.real_label)\n",
        "        self.err_d_fake = self.l_bce(self.pred_fake, self.fake_label)\n",
        "\n",
        "        # NetD Loss & Backward-Pass\n",
        "        self.err_d = (self.err_d_real + self.err_d_fake) * 0.5\n",
        "        self.err_d.backward()\n",
        "\n",
        "    def reinit_d(self):\n",
        "        self.netd.apply(weights_init)\n",
        "        print('   Reloading net d')\n",
        "\n",
        "    def optimize_params(self):\n",
        "        # Forward-pass\n",
        "        self.forward_g()\n",
        "        self.forward_d()\n",
        "\n",
        "        # Backward-pass\n",
        "        # netg\n",
        "        self.optimizer_g.zero_grad()\n",
        "        self.backward_g()\n",
        "        self.optimizer_g.step()\n",
        "\n",
        "        # netd\n",
        "        self.optimizer_d.zero_grad()\n",
        "        self.backward_d()\n",
        "        self.optimizer_d.step()\n",
        "        if self.err_d.item() < 1e-5: self.reinit_d()\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        old_lr = self.optimizer_d.param_groups[0]['lr']\n",
        "        self.scheduler_d.step()\n",
        "        self.scheduler_g.step()\n",
        "        lr = self.optimizer_d.param_groups[0]['lr']\n",
        "        print('learning rate %.7f --> %.7f' % (old_lr, lr))\n",
        "    \n",
        "    def normalize(self,inp):\n",
        "        return (inp - inp.min()) / (inp.max() - inp.min() + 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy8Nasb5RhTr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6A5l4ywueKT"
      },
      "source": [
        "opt.isTrain = True\n",
        "train_loader, valid_loader = get_dataloaders(opt.train_data_path, opt.validation_data_path)\n",
        "model = Ganomaly(train_loader, valid_loader)\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMydYtCqgux3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac6dec0-d7d3-4ddb-9244-be1d3b0c9d33"
      },
      "source": [
        "# best epoch confirmation\r\n",
        "opt.isTrain = False\r\n",
        "train_loader, test_loader = get_dataloaders(opt.train_data_path, opt.validation_data_path)\r\n",
        "print(len(train_loader), len(test_loader))\r\n",
        "model = Ganomaly(train_loader, test_loader)\r\n",
        "model.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1106 169\n",
            "   Loaded weights.\n",
            "169 0.003091573715209961 0.0023164749145507812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('Avg Run Time (ms/batch)', 2.544858455657959),\n",
              "             ('roc', 0.983692393631878)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EESLCv8ig5Hx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b73cd9-75c6-48de-c3f0-e9c3acf7a25d"
      },
      "source": [
        "max(model.an_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7859, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCZWfdXYRjTN"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAbx08BDYhLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff82504-be2f-4fad-e07e-f4cdf40324b7"
      },
      "source": [
        "opt.isTrain = False\n",
        "train_loader, test_loader = get_dataloaders(opt.train_data_path, opt.test_data_path)\n",
        "print(len(train_loader), len(test_loader))\n",
        "model = Ganomaly(train_loader, test_loader)\n",
        "model.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1106 1344\n",
            "   Loaded weights.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('Avg Run Time (ms/batch)', 2.770254611968994),\n",
              "             ('roc', 0.8925645470630403)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSruyLm3g8tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3934c87-2f12-4ac5-def3-efab2135dc98"
      },
      "source": [
        "max(model.an_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5583, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZcs2FUh2gV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310dff08-05ee-4d68-9b8d-9bfa3d8ddc0d"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JALoxJsFSDBG"
      },
      "source": [
        "# Testing per Patient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGycX_qDMHLO"
      },
      "source": [
        "# test on jsrt patients\n",
        "opt.isTrain = False\n",
        "test_patients = [patients[0], patients[196]]\n",
        "model.test_patients(test_patients, [1.,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rWfRwZWipjG"
      },
      "source": [
        "max(test_patients[0].anon_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw0RiCPJMKUO"
      },
      "source": [
        "for patient in test_patients:\n",
        "    patient.display_anon_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTuNN68uda1w"
      },
      "source": [
        "dataset = datasets.ImageFolder(\"data/patients/JPCLN001/\")\r\n",
        "for el in dataset:\r\n",
        "    print(el[0].name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVnWcBKx3TfF"
      },
      "source": [
        "# Environment Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbZuvfv33PK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e150c02-31f0-4192-d645-6ad909b5e286"
      },
      "source": [
        "# This script outputs relevant system environment info\r\n",
        "# Run it with `python collect_env.py`.\r\n",
        "import locale\r\n",
        "import re\r\n",
        "import subprocess\r\n",
        "import sys\r\n",
        "import os\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "try:\r\n",
        "    import torch\r\n",
        "    TORCH_AVAILABLE = True\r\n",
        "except (ImportError, NameError, AttributeError, OSError):\r\n",
        "    TORCH_AVAILABLE = False\r\n",
        "\r\n",
        "# System Environment Information\r\n",
        "SystemEnv = namedtuple('SystemEnv', [\r\n",
        "    'torch_version',\r\n",
        "    'is_debug_build',\r\n",
        "    'cuda_compiled_version',\r\n",
        "    'gcc_version',\r\n",
        "    'clang_version',\r\n",
        "    'cmake_version',\r\n",
        "    'os',\r\n",
        "    'python_version',\r\n",
        "    'is_cuda_available',\r\n",
        "    'cuda_runtime_version',\r\n",
        "    'nvidia_driver_version',\r\n",
        "    'nvidia_gpu_models',\r\n",
        "    'cudnn_version',\r\n",
        "    'pip_version',  # 'pip' or 'pip3'\r\n",
        "    'pip_packages',\r\n",
        "    'conda_packages',\r\n",
        "    'hip_compiled_version',\r\n",
        "    'hip_runtime_version',\r\n",
        "    'miopen_runtime_version',\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "def run(command):\r\n",
        "    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\r\n",
        "    p = subprocess.Popen(command, stdout=subprocess.PIPE,\r\n",
        "                         stderr=subprocess.PIPE, shell=True)\r\n",
        "    raw_output, raw_err = p.communicate()\r\n",
        "    rc = p.returncode\r\n",
        "    if get_platform() == 'win32':\r\n",
        "        enc = 'oem'\r\n",
        "    else:\r\n",
        "        enc = locale.getpreferredencoding()\r\n",
        "    output = raw_output.decode(enc)\r\n",
        "    err = raw_err.decode(enc)\r\n",
        "    return rc, output.strip(), err.strip()\r\n",
        "\r\n",
        "\r\n",
        "def run_and_read_all(run_lambda, command):\r\n",
        "    \"\"\"Runs command using run_lambda; reads and returns entire output if rc is 0\"\"\"\r\n",
        "    rc, out, _ = run_lambda(command)\r\n",
        "    if rc != 0:\r\n",
        "        return None\r\n",
        "    return out\r\n",
        "\r\n",
        "\r\n",
        "def run_and_parse_first_match(run_lambda, command, regex):\r\n",
        "    \"\"\"Runs command using run_lambda, returns the first regex match if it exists\"\"\"\r\n",
        "    rc, out, _ = run_lambda(command)\r\n",
        "    if rc != 0:\r\n",
        "        return None\r\n",
        "    match = re.search(regex, out)\r\n",
        "    if match is None:\r\n",
        "        return None\r\n",
        "    return match.group(1)\r\n",
        "\r\n",
        "\r\n",
        "def get_conda_packages(run_lambda):\r\n",
        "    if get_platform() == 'win32':\r\n",
        "        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\r\n",
        "        findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\r\n",
        "        grep_cmd = r'{} /R \"torch numpy cudatoolkit soumith mkl magma\"'.format(findstr_cmd)\r\n",
        "    else:\r\n",
        "        grep_cmd = r'grep \"torch\\|numpy\\|cudatoolkit\\|soumith\\|mkl\\|magma\"'\r\n",
        "    conda = os.environ.get('CONDA_EXE', 'conda')\r\n",
        "    out = run_and_read_all(run_lambda, conda + ' list | ' + grep_cmd)\r\n",
        "    if out is None:\r\n",
        "        return out\r\n",
        "    # Comment starting at beginning of line\r\n",
        "    comment_regex = re.compile(r'^#.*\\n')\r\n",
        "    return re.sub(comment_regex, '', out)\r\n",
        "\r\n",
        "\r\n",
        "def get_gcc_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')\r\n",
        "\r\n",
        "def get_clang_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'clang --version', r'clang version (.*)')\r\n",
        "\r\n",
        "\r\n",
        "def get_cmake_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'cmake --version', r'cmake (.*)')\r\n",
        "\r\n",
        "\r\n",
        "def get_nvidia_driver_version(run_lambda):\r\n",
        "    if get_platform() == 'darwin':\r\n",
        "        cmd = 'kextstat | grep -i cuda'\r\n",
        "        return run_and_parse_first_match(run_lambda, cmd,\r\n",
        "                                         r'com[.]nvidia[.]CUDA [(](.*?)[)]')\r\n",
        "    smi = get_nvidia_smi()\r\n",
        "    return run_and_parse_first_match(run_lambda, smi, r'Driver Version: (.*?) ')\r\n",
        "\r\n",
        "\r\n",
        "def get_gpu_info(run_lambda):\r\n",
        "    if get_platform() == 'darwin' or (TORCH_AVAILABLE and hasattr(torch.version, 'hip') and torch.version.hip is not None):\r\n",
        "        if TORCH_AVAILABLE and torch.cuda.is_available():\r\n",
        "            return torch.cuda.get_device_name(None)\r\n",
        "        return None\r\n",
        "    smi = get_nvidia_smi()\r\n",
        "    uuid_regex = re.compile(r' \\(UUID: .+?\\)')\r\n",
        "    rc, out, _ = run_lambda(smi + ' -L')\r\n",
        "    if rc != 0:\r\n",
        "        return None\r\n",
        "    # Anonymize GPUs by removing their UUID\r\n",
        "    return re.sub(uuid_regex, '', out)\r\n",
        "\r\n",
        "\r\n",
        "def get_running_cuda_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'nvcc --version', r'release .+ V(.*)')\r\n",
        "\r\n",
        "\r\n",
        "def get_cudnn_version(run_lambda):\r\n",
        "    \"\"\"This will return a list of libcudnn.so; it's hard to tell which one is being used\"\"\"\r\n",
        "    if get_platform() == 'win32':\r\n",
        "        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\r\n",
        "        cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\r\n",
        "        where_cmd = os.path.join(system_root, 'System32', 'where')\r\n",
        "        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\r\n",
        "    elif get_platform() == 'darwin':\r\n",
        "        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\r\n",
        "        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\r\n",
        "        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\r\n",
        "        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\r\n",
        "        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'\r\n",
        "    else:\r\n",
        "        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d\" \" -f1 | rev'\r\n",
        "    rc, out, _ = run_lambda(cudnn_cmd)\r\n",
        "    # find will return 1 if there are permission errors or if not found\r\n",
        "    if len(out) == 0 or (rc != 1 and rc != 0):\r\n",
        "        l = os.environ.get('CUDNN_LIBRARY')\r\n",
        "        if l is not None and os.path.isfile(l):\r\n",
        "            return os.path.realpath(l)\r\n",
        "        return None\r\n",
        "    files_set = set()\r\n",
        "    for fn in out.split('\\n'):\r\n",
        "        fn = os.path.realpath(fn)  # eliminate symbolic links\r\n",
        "        if os.path.isfile(fn):\r\n",
        "            files_set.add(fn)\r\n",
        "    if not files_set:\r\n",
        "        return None\r\n",
        "    # Alphabetize the result because the order is non-deterministic otherwise\r\n",
        "    files = list(sorted(files_set))\r\n",
        "    if len(files) == 1:\r\n",
        "        return files[0]\r\n",
        "    result = '\\n'.join(files)\r\n",
        "    return 'Probably one of the following:\\n{}'.format(result)\r\n",
        "\r\n",
        "\r\n",
        "def get_nvidia_smi():\r\n",
        "    # Note: nvidia-smi is currently available only on Windows and Linux\r\n",
        "    smi = 'nvidia-smi'\r\n",
        "    if get_platform() == 'win32':\r\n",
        "        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\r\n",
        "        program_files_root = os.environ.get('PROGRAMFILES', 'C:\\\\Program Files')\r\n",
        "        legacy_path = os.path.join(program_files_root, 'NVIDIA Corporation', 'NVSMI', smi)\r\n",
        "        new_path = os.path.join(system_root, 'System32', smi)\r\n",
        "        smis = [new_path, legacy_path]\r\n",
        "        for candidate_smi in smis:\r\n",
        "            if os.path.exists(candidate_smi):\r\n",
        "                smi = f'\"{candidate_smi}\"'\r\n",
        "                break\r\n",
        "    return smi\r\n",
        "\r\n",
        "\r\n",
        "def get_platform():\r\n",
        "    if sys.platform.startswith('linux'):\r\n",
        "        return 'linux'\r\n",
        "    elif sys.platform.startswith('win32'):\r\n",
        "        return 'win32'\r\n",
        "    elif sys.platform.startswith('cygwin'):\r\n",
        "        return 'cygwin'\r\n",
        "    elif sys.platform.startswith('darwin'):\r\n",
        "        return 'darwin'\r\n",
        "    else:\r\n",
        "        return sys.platform\r\n",
        "\r\n",
        "\r\n",
        "def get_mac_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion', r'(.*)')\r\n",
        "\r\n",
        "\r\n",
        "def get_windows_version(run_lambda):\r\n",
        "    system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\r\n",
        "    wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\r\n",
        "    findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\r\n",
        "    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\r\n",
        "\r\n",
        "\r\n",
        "def get_lsb_version(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'lsb_release -a', r'Description:\\t(.*)')\r\n",
        "\r\n",
        "\r\n",
        "def check_release_file(run_lambda):\r\n",
        "    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',\r\n",
        "                                     r'PRETTY_NAME=\"(.*)\"')\r\n",
        "\r\n",
        "\r\n",
        "def get_os(run_lambda):\r\n",
        "    from platform import machine\r\n",
        "    platform = get_platform()\r\n",
        "\r\n",
        "    if platform == 'win32' or platform == 'cygwin':\r\n",
        "        return get_windows_version(run_lambda)\r\n",
        "\r\n",
        "    if platform == 'darwin':\r\n",
        "        version = get_mac_version(run_lambda)\r\n",
        "        if version is None:\r\n",
        "            return None\r\n",
        "        return 'macOS {} ({})'.format(version, machine())\r\n",
        "\r\n",
        "    if platform == 'linux':\r\n",
        "        # Ubuntu/Debian based\r\n",
        "        desc = get_lsb_version(run_lambda)\r\n",
        "        if desc is not None:\r\n",
        "            return '{} ({})'.format(desc, machine())\r\n",
        "\r\n",
        "        # Try reading /etc/*-release\r\n",
        "        desc = check_release_file(run_lambda)\r\n",
        "        if desc is not None:\r\n",
        "            return '{} ({})'.format(desc, machine())\r\n",
        "\r\n",
        "        return '{} ({})'.format(platform, machine())\r\n",
        "\r\n",
        "    # Unknown platform\r\n",
        "    return platform\r\n",
        "\r\n",
        "\r\n",
        "def get_pip_packages(run_lambda):\r\n",
        "    \"\"\"Returns `pip list` output. Note: will also find conda-installed pytorch\r\n",
        "    and numpy packages.\"\"\"\r\n",
        "    # People generally have `pip` as `pip` or `pip3`\r\n",
        "    def run_with_pip(pip):\r\n",
        "        if get_platform() == 'win32':\r\n",
        "            system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\r\n",
        "            findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\r\n",
        "            grep_cmd = r'{} /R \"numpy torch\"'.format(findstr_cmd)\r\n",
        "        else:\r\n",
        "            grep_cmd = r'grep \"torch\\|numpy\"'\r\n",
        "        return run_and_read_all(run_lambda, pip + ' list --format=freeze | ' + grep_cmd)\r\n",
        "\r\n",
        "    # Try to figure out if the user is running pip or pip3.\r\n",
        "    out2 = run_with_pip('pip')\r\n",
        "    out3 = run_with_pip('pip3')\r\n",
        "\r\n",
        "    num_pips = len([x for x in [out2, out3] if x is not None])\r\n",
        "    if num_pips == 0:\r\n",
        "        return 'pip', out2\r\n",
        "\r\n",
        "    if num_pips == 1:\r\n",
        "        if out2 is not None:\r\n",
        "            return 'pip', out2\r\n",
        "        return 'pip3', out3\r\n",
        "\r\n",
        "    # num_pips is 2. Return pip3 by default b/c that most likely\r\n",
        "    # is the one associated with Python 3\r\n",
        "    return 'pip3', out3\r\n",
        "\r\n",
        "\r\n",
        "def get_env_info():\r\n",
        "    run_lambda = run\r\n",
        "    pip_version, pip_list_output = get_pip_packages(run_lambda)\r\n",
        "\r\n",
        "    if TORCH_AVAILABLE:\r\n",
        "        version_str = torch.__version__\r\n",
        "        debug_mode_str = str(torch.version.debug)\r\n",
        "        cuda_available_str = str(torch.cuda.is_available())\r\n",
        "        cuda_version_str = torch.version.cuda\r\n",
        "        if not hasattr(torch.version, 'hip') or torch.version.hip is None:  # cuda version\r\n",
        "            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\r\n",
        "        else:  # HIP version\r\n",
        "            cfg = torch._C._show_config().split('\\n')\r\n",
        "            hip_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'HIP Runtime' in s][0]\r\n",
        "            miopen_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'MIOpen' in s][0]\r\n",
        "            cuda_version_str = 'N/A'\r\n",
        "            hip_compiled_version = torch.version.hip\r\n",
        "    else:\r\n",
        "        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\r\n",
        "        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\r\n",
        "\r\n",
        "    return SystemEnv(\r\n",
        "        torch_version=version_str,\r\n",
        "        is_debug_build=debug_mode_str,\r\n",
        "        python_version='{}.{} ({}-bit runtime)'.format(sys.version_info[0], sys.version_info[1], sys.maxsize.bit_length() + 1),\r\n",
        "        is_cuda_available=cuda_available_str,\r\n",
        "        cuda_compiled_version=cuda_version_str,\r\n",
        "        cuda_runtime_version=get_running_cuda_version(run_lambda),\r\n",
        "        nvidia_gpu_models=get_gpu_info(run_lambda),\r\n",
        "        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\r\n",
        "        cudnn_version=get_cudnn_version(run_lambda),\r\n",
        "        hip_compiled_version=hip_compiled_version,\r\n",
        "        hip_runtime_version=hip_runtime_version,\r\n",
        "        miopen_runtime_version=miopen_runtime_version,\r\n",
        "        pip_version=pip_version,\r\n",
        "        pip_packages=pip_list_output,\r\n",
        "        conda_packages=get_conda_packages(run_lambda),\r\n",
        "        os=get_os(run_lambda),\r\n",
        "        gcc_version=get_gcc_version(run_lambda),\r\n",
        "        clang_version=get_clang_version(run_lambda),\r\n",
        "        cmake_version=get_cmake_version(run_lambda),\r\n",
        "    )\r\n",
        "\r\n",
        "env_info_fmt = \"\"\"\r\n",
        "PyTorch version: {torch_version}\r\n",
        "Is debug build: {is_debug_build}\r\n",
        "CUDA used to build PyTorch: {cuda_compiled_version}\r\n",
        "ROCM used to build PyTorch: {hip_compiled_version}\r\n",
        "OS: {os}\r\n",
        "GCC version: {gcc_version}\r\n",
        "Clang version: {clang_version}\r\n",
        "CMake version: {cmake_version}\r\n",
        "Python version: {python_version}\r\n",
        "Is CUDA available: {is_cuda_available}\r\n",
        "CUDA runtime version: {cuda_runtime_version}\r\n",
        "GPU models and configuration: {nvidia_gpu_models}\r\n",
        "Nvidia driver version: {nvidia_driver_version}\r\n",
        "cuDNN version: {cudnn_version}\r\n",
        "HIP runtime version: {hip_runtime_version}\r\n",
        "MIOpen runtime version: {miopen_runtime_version}\r\n",
        "Versions of relevant libraries:\r\n",
        "{pip_packages}\r\n",
        "{conda_packages}\r\n",
        "\"\"\".strip()\r\n",
        "\r\n",
        "\r\n",
        "def pretty_str(envinfo):\r\n",
        "    def replace_nones(dct, replacement='Could not collect'):\r\n",
        "        for key in dct.keys():\r\n",
        "            if dct[key] is not None:\r\n",
        "                continue\r\n",
        "            dct[key] = replacement\r\n",
        "        return dct\r\n",
        "\r\n",
        "    def replace_bools(dct, true='Yes', false='No'):\r\n",
        "        for key in dct.keys():\r\n",
        "            if dct[key] is True:\r\n",
        "                dct[key] = true\r\n",
        "            elif dct[key] is False:\r\n",
        "                dct[key] = false\r\n",
        "        return dct\r\n",
        "\r\n",
        "    def prepend(text, tag='[prepend]'):\r\n",
        "        lines = text.split('\\n')\r\n",
        "        updated_lines = [tag + line for line in lines]\r\n",
        "        return '\\n'.join(updated_lines)\r\n",
        "\r\n",
        "    def replace_if_empty(text, replacement='No relevant packages'):\r\n",
        "        if text is not None and len(text) == 0:\r\n",
        "            return replacement\r\n",
        "        return text\r\n",
        "\r\n",
        "    def maybe_start_on_next_line(string):\r\n",
        "        # If `string` is multiline, prepend a \\n to it.\r\n",
        "        if string is not None and len(string.split('\\n')) > 1:\r\n",
        "            return '\\n{}\\n'.format(string)\r\n",
        "        return string\r\n",
        "\r\n",
        "    mutable_dict = envinfo._asdict()\r\n",
        "\r\n",
        "    # If nvidia_gpu_models is multiline, start on the next line\r\n",
        "    mutable_dict['nvidia_gpu_models'] = \\\r\n",
        "        maybe_start_on_next_line(envinfo.nvidia_gpu_models)\r\n",
        "\r\n",
        "    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\r\n",
        "    dynamic_cuda_fields = [\r\n",
        "        'cuda_runtime_version',\r\n",
        "        'nvidia_gpu_models',\r\n",
        "        'nvidia_driver_version',\r\n",
        "    ]\r\n",
        "    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']\r\n",
        "    all_dynamic_cuda_fields_missing = all(\r\n",
        "        mutable_dict[field] is None for field in dynamic_cuda_fields)\r\n",
        "    if TORCH_AVAILABLE and not torch.cuda.is_available() and all_dynamic_cuda_fields_missing:\r\n",
        "        for field in all_cuda_fields:\r\n",
        "            mutable_dict[field] = 'No CUDA'\r\n",
        "        if envinfo.cuda_compiled_version is None:\r\n",
        "            mutable_dict['cuda_compiled_version'] = 'None'\r\n",
        "\r\n",
        "    # Replace True with Yes, False with No\r\n",
        "    mutable_dict = replace_bools(mutable_dict)\r\n",
        "\r\n",
        "    # Replace all None objects with 'Could not collect'\r\n",
        "    mutable_dict = replace_nones(mutable_dict)\r\n",
        "\r\n",
        "    # If either of these are '', replace with 'No relevant packages'\r\n",
        "    mutable_dict['pip_packages'] = replace_if_empty(mutable_dict['pip_packages'])\r\n",
        "    mutable_dict['conda_packages'] = replace_if_empty(mutable_dict['conda_packages'])\r\n",
        "\r\n",
        "    # Tag conda and pip packages with a prefix\r\n",
        "    # If they were previously None, they'll show up as ie '[conda] Could not collect'\r\n",
        "    if mutable_dict['pip_packages']:\r\n",
        "        mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\r\n",
        "                                               '[{}] '.format(envinfo.pip_version))\r\n",
        "    if mutable_dict['conda_packages']:\r\n",
        "        mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\r\n",
        "                                                 '[conda] ')\r\n",
        "    return env_info_fmt.format(**mutable_dict)\r\n",
        "\r\n",
        "\r\n",
        "def get_pretty_env_info():\r\n",
        "    return pretty_str(get_env_info())\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    print(\"Collecting environment information...\")\r\n",
        "    output = get_pretty_env_info()\r\n",
        "    print(output)\r\n",
        "\r\n",
        "\r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting environment information...\n",
            "PyTorch version: 1.7.0+cu101\n",
            "Is debug build: True\n",
            "CUDA used to build PyTorch: 10.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "OS: Ubuntu 18.04.5 LTS (x86_64)\n",
            "GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)\n",
            "CMake version: version 3.12.0\n",
            "Python version: 3.6 (64-bit runtime)\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 10.1.243\n",
            "GPU models and configuration: GPU 0: Tesla P100-PCIE-16GB\n",
            "Nvidia driver version: 460.32.03\n",
            "cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Versions of relevant libraries:\n",
            "[pip3] numpy==1.19.5\n",
            "[pip3] pytorch-msssim==0.2.1\n",
            "[pip3] torch==1.7.0+cu101\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtext==0.3.1\n",
            "[pip3] torchvision==0.8.1+cu101\n",
            "[conda] Could not collect\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}