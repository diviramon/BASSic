{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PanoGAN_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diviramon/BASSic/blob/master/PanoGAN_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7zy0qm4qrci",
        "outputId": "81127025-b334-4dfa-f218-530f19e30e72"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar 12 23:29:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi37gsnXq6dh",
        "outputId": "1b5e428b-1b86-4c83-e56b-45377848d002"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qqc8FvuMVBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aae5387-39aa-4eb8-e783-f9d8314c3bf3"
      },
      "source": [
        "!pip install pydicom\n",
        "!pip install pytorch-msssim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n",
            "Collecting pytorch-msssim\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/d3/3cb0f397232cf79e1762323c3a8862e39ad53eca0bb5f6be9ccc8e7c070e/pytorch_msssim-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-msssim) (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-msssim) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-msssim) (1.19.5)\n",
            "Installing collected packages: pytorch-msssim\n",
            "Successfully installed pytorch-msssim-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lW9VN4GBSr3"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SgV287Q7i4"
      },
      "source": [
        "DIM = 64\n",
        "OUTPUT_DIM = DIM*DIM*1\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import random\n",
        "from skimage.io import imread \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.autograd import grad\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import random_split, ConcatDataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, f1_score\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pydicom as dcm\n",
        "from skimage import io, exposure\n",
        "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import autograd\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import sampler\n",
        "from argparse import ArgumentParser\n",
        "from sklearn import metrics"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-uHk07CBVK5"
      },
      "source": [
        "# Make Folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1w65QGS-dm"
      },
      "source": [
        "!rm -r sample_data/\n",
        "!mkdir data\n",
        "!mkdir data/train\n",
        "!mkdir data/train/healthy\n",
        "!mkdir data/valid\n",
        "!mkdir data/valid/healthy\n",
        "!mkdir data/valid/nodule\n",
        "!mkdir data/test/\n",
        "!mkdir data/test/healthy\n",
        "!mkdir data/test/nodule\n",
        "!mkdir data/patients/\n",
        "!mkdir output/\n",
        "!mkdir output/patients/\n",
        "\n",
        "!mkdir encoder"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UuoCc3eBXQa"
      },
      "source": [
        "# Read Image Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6ev8OpNLXKr"
      },
      "source": [
        "def read_indiana(file_name, cxr_path, mask_path):\n",
        "  \n",
        "  dcm_filename = file_name[3:-4] + '.dcm'\n",
        "\n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "\n",
        "  # Read in DICOM \n",
        "  ds = dcm.dcmread(os.path.join(cxr_path, dcm_filename))\n",
        "  cxr = ds.pixel_array\n",
        "  #print(ds.PhotometricInterpretation)\n",
        "  #cxr = dcm.pixel_data_handlers.util.apply_voi_lut(cxr, ds, index=0)\n",
        "  cxr = cv2.resize(cxr, (1024, 1024))\n",
        "  if ds.PhotometricInterpretation == 'MONOCHROME1':\n",
        "    cxr = 1.0 - cxr * 1./cxr.max()\n",
        "  else:\n",
        "    cxr = cxr * 1. / cxr.max()\n",
        "  cxr = exposure.equalize_adapthist(cxr,kernel_size=128)\n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODytx0lWLXxp"
      },
      "source": [
        "def read_nih(file_name, cxr_path, mask_path):\n",
        "  \n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "  \n",
        "  # Read in PNG\n",
        "  cxr = cv2.imread(os.path.join(cxr_path, file_name))\n",
        "  cxr = cv2.cvtColor(cxr, cv2.COLOR_BGR2GRAY)\n",
        "  cxr = cv2.resize(cxr, (1024, 1024)) \n",
        "  cxr = cxr * 1./cxr.max()\n",
        "  cxr = exposure.equalize_adapthist(cxr,kernel_size=128)\n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD5nusdNLaW8"
      },
      "source": [
        "def read_jsrt(file_name, cxr_path, mask_path):\n",
        "\n",
        "  # Pre-process mask\n",
        "  mask = cv2.imread(os.path.join(mask_path, file_name))\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
        "  mask = (mask > 127) * 255\n",
        "\n",
        "  # Read in IMG\n",
        "  # fname = file_name[:-4] + '.IMG'\n",
        "  # cxr = 1.0 - np.fromfile(os.path.join(cxr_path, fname), dtype='>u2').reshape((2048, 2048)) * 1. / 4095\n",
        "  cxr = cv2.imread(os.path.join(cxr_path, file_name))\n",
        "  cxr = cv2.cvtColor(cxr, cv2.COLOR_BGR2GRAY)\n",
        "  cxr = cv2.resize(cxr, (1024, 1024)) \n",
        "  cxr = cxr * 1./cxr.max()\n",
        "  cxr = exposure.equalize_adapthist(cxr,kernel_size=128)\n",
        "  normalizedImg = np.zeros((1024, 1024))\n",
        "  cxr = cv2.normalize(cxr,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  return mask, cxr"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNXphgg8Lb3O"
      },
      "source": [
        "def crop_patches(image, mask, total_patches, box, image_name, dataset, svpath):\n",
        "  if dataset == 'indiana':\n",
        "    patch_extractor = ExtractPatches(image_name, image, mask, box, DIM, DIM//2, dataset, svpath)\n",
        "  else:\n",
        "    patch_extractor = ExtractPatches(image_name, image, mask, box, DIM, DIM//4, dataset, svpath)\n",
        "  \n",
        "  lung_area = np.count_nonzero(mask)\n",
        "  patches = patch_extractor.extract_all_patches()\n",
        "  total_patches = total_patches + patches\n",
        "\n",
        "  return total_patches"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLQTelfzLdvf"
      },
      "source": [
        "class ExtractPatches:\n",
        "    def __init__(self, image_name, image, mask, bbox, patchSize, stride, dataset, out_path):\n",
        "        self.image = image\n",
        "        self.out_path = out_path\n",
        "        self.dataset = dataset\n",
        "        self.bbox = bbox\n",
        "        self.annotated = np.copy(image) \n",
        "        self.reconstructed = np.empty(image.shape)\n",
        "        self.reconstructed[:] = np.nan\n",
        "        self.image_name = image_name\n",
        "        self.patchSize = patchSize\n",
        "        self.stride = stride\n",
        "        self.mask = mask\n",
        "\n",
        "    def draw_single_patches(self, coords, patch):\n",
        "      y1 = coords[0] * self.stride\n",
        "      x1 = coords[1] * self.stride\n",
        "      y2 = coords[0] * self.stride + self.patchSize\n",
        "      x2 = coords[1] * self.stride + self.patchSize\n",
        "      self.annotated = cv2.rectangle(self.annotated,(x1,y1), (x2,y2), (0, 0, 255))\n",
        "      self.reconstructed[y1:y2, x1:x2] = patch\n",
        "    \n",
        "    def extract_single_patches(self, patch):\n",
        "        croppedPatches = self.image[(patch[0] * self.stride):(patch[0] * self.stride + self.patchSize), \n",
        "                               (patch[1] * self.stride):(patch[1] * self.stride + self.patchSize)]\n",
        "        return croppedPatches\n",
        "\n",
        "    def no_of_patches(self):\n",
        "        yNoOfPatches, xNoOfPatches = (int((self.image.shape[1] - self.patchSize) / self.stride + 1),\n",
        "                                      int((self.image.shape[0] - self.patchSize) / self.stride + 1))\n",
        "        return xNoOfPatches, yNoOfPatches\n",
        "\n",
        "    def extract_all_patches(self):\n",
        "        xNoOfPatches, yNoOfPatches = self.no_of_patches()\n",
        "        closest = None\n",
        "        closest_dist = 10000\n",
        "\n",
        "        allPatches = list()\n",
        "        for y in range(yNoOfPatches):\n",
        "            for x in range(xNoOfPatches):\n",
        "              patch = self.extract_single_patches((x,y))\n",
        "                            \n",
        "              y1 = x * self.stride\n",
        "              x1 = y * self.stride\n",
        "              y2 = x * self.stride + self.patchSize\n",
        "              x2 = y * self.stride + self.patchSize\n",
        "              mask_cover = np.sum(self.mask[y1:y2, x1:x2])\n",
        "              if x1 > 0 and x2 < 1024 and y1 > 0 and y2 < 1024:\n",
        "                if patch.shape[0] == self.patchSize and patch.shape[1] and mask_cover/(255*self.patchSize*self.patchSize) > 0.75:\n",
        "                  if self.dataset == 'jsrt-nodule':\n",
        "                      cx = self.bbox[0]\n",
        "                      cy = self.bbox[1]\n",
        "                      if (x1 < cx) and (x2 > cx) and (y1 < cy) and (y2 > cy):\n",
        "                          # bbox_cx = (x1+x2)//2\n",
        "                          # bbox_cy = (y1+y2)//2\n",
        "                          # if ((bbox_cx-cx)**2 + (bbox_cy-cy)**2) < closest_dist:\n",
        "                          #     closest_dist = (bbox_cx-cx)**2 + (bbox_cy-cy)**2\n",
        "                          #     closest = patch\n",
        "                          #     closest_x = x\n",
        "                          #     closest_y = y\n",
        "                          # self.draw_single_patches((x,y), patch)\n",
        "                          cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                          self.draw_single_patches((x,y), patch)\n",
        "                  elif self.dataset == 'nih-nodule':\n",
        "                      cx = (self.bbox[0] + self.bbox[2])//2\n",
        "                      cy = (self.bbox[1] + self.bbox[3])//2\n",
        "                      if (x1 < cx) and (x2 > cx) and (y1 < cy) and (y2 > cy):\n",
        "                          # bbox_cx = (x1+x2)//2\n",
        "                          # bbox_cy = (y1+y2)//2\n",
        "                          # if ((bbox_cx-cx)**2 + (bbox_cy-cy)**2) < closest_dist:\n",
        "                          #     closest_dist = (bbox_cx-cx)**2 + (bbox_cy-cy)**2\n",
        "                          #     closest = patch\n",
        "                          #     closest_x = x\n",
        "                          #     closest_y = y\n",
        "                          # self.draw_single_patches((x,y), patch)\n",
        "                          cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                          self.draw_single_patches((x,y), patch)\n",
        "                  else:\n",
        "                      allPatches.append(patch)\n",
        "                      cv2.imwrite(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch)\n",
        "                      # plt.imsave(self.out_path + str(x) + '_' + str(y) + '_' + self.image_name, patch, cmap='gray')\n",
        "                      self.draw_single_patches((x,y), patch)\n",
        "        \n",
        "        if closest is not None:\n",
        "            allPatches.append(closest)\n",
        "            cv2.imwrite(self.out_path + str(closest_x) + '_' + str(closest_y) + '_' + self.image_name, closest)\n",
        "            # plt.imsave(self.out_path + str(closest_x) + '_' + str(closest_y) + '_' + self.image_name, closest, cmap='gray')\n",
        "\n",
        "        #cv2_imshow(self.annotated)\n",
        "\n",
        "        return allPatches"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRjrn3yBc15"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefDiouvLflW"
      },
      "source": [
        "jsrt_mask_path = '/content/drive/MyDrive/FYDP 2021/JSRT Data/JSRT Masks'\n",
        "jsrt_cxr_path = '/content/drive/MyDrive/FYDP 2021/JSRT Data/JSRT Images'\n",
        "\n",
        "nih_healthy_mask_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Healthy_masks'\n",
        "nih_healthy_cxr_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Healthy_raw'\n",
        "\n",
        "nih_nodule_mask_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Nodule_masks'\n",
        "nih_nodule_cxr_path = '/content/drive/MyDrive/FYDP 2021/NIH-Data/Nodule_raw'\n",
        "\n",
        "indiana_mask_path = '/content/drive/My Drive/FYDP 2021/Indiana University Database/Additional Masks'\n",
        "indiana_cxr_path = '/content/drive/My Drive/FYDP 2021/Indiana University Database/DICOM Frontal'\n",
        "\n",
        "nih_healthy_img_list = os.listdir(nih_healthy_mask_path)\n",
        "indiana_img_list = os.listdir(indiana_mask_path)\n",
        "\n",
        "nih_info = pd.read_csv('/content/drive/MyDrive/FYDP 2021/NIH-Data/BBox_List_2017.csv')\n",
        "nih_info = nih_info[nih_info['Finding Label']=='Nodule']\n",
        "nih_nodule_list = nih_info['Image Index'].values\n",
        "\n",
        "jsrt_info = pd.read_csv('/content/drive/MyDrive/FYDP 2021/JSRT Data/jsrt_metadata (1).csv')\n",
        "jsrt_healthy_info = jsrt_info[jsrt_info['state']=='non-nodule']\n",
        "jsrt_healthy_list = jsrt_healthy_info['study_id'].values\n",
        "jsrt_nodule_info = jsrt_info[jsrt_info['state']!='non-nodule']\n",
        "jsrt_nodule_list = jsrt_nodule_info['study_id'].values\n",
        "\n",
        "batchsize = 64\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG0CQWQ2BeoR"
      },
      "source": [
        "# Load Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrd3ASF0Bhs8"
      },
      "source": [
        "### Indiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6S410tvLv3t",
        "outputId": "b281d395-5cfb-433b-a3f0-649ade06c11f"
      },
      "source": [
        "total_patches = []\n",
        "for j in range(len(indiana_img_list)):\n",
        "    image_name = indiana_img_list[j]\n",
        "    #print(image_name)\n",
        "    box = []\n",
        "    mask, cxr = read_indiana(image_name, indiana_cxr_path, indiana_mask_path)\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'indiana', 'data/train/healthy/')\n",
        "len(total_patches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RUXmHDRBjgQ"
      },
      "source": [
        "### NIH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTI57h0VLxPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca9d183-c420-4612-e628-3b7eacc2dc86"
      },
      "source": [
        "total_patches = []\n",
        "for j in range(len(nih_healthy_img_list)):\n",
        "    image_name = nih_healthy_img_list[j]\n",
        "    #print(image_name)\n",
        "    box = []\n",
        "    mask, cxr = read_nih(image_name, nih_healthy_cxr_path, nih_healthy_mask_path)\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'nih-healthy', 'data/valid/healthy/')\n",
        "len(total_patches)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9010"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKWa5njFLy7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb652d1-10d6-4477-da71-5b548b4994d7"
      },
      "source": [
        "total_patches = []\n",
        "for j in range(len(nih_nodule_list)):\n",
        "    image_name = nih_nodule_list[j]\n",
        "    #print(image_name)\n",
        "    bx = nih_info['Bbox [x'].values[j]\n",
        "    by = nih_info['y'].values[j]\n",
        "    bh = nih_info['h]'].values[j]\n",
        "    bw = nih_info['w'].values[j]\n",
        "    box = [bx,by, bx+bw, by+bh]\n",
        "    mask, cxr = read_nih(image_name, nih_nodule_cxr_path, nih_nodule_mask_path)\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'nih-nodule', 'data/valid/nodule/')\n",
        "len(total_patches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPcE_L6gL01i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2367ef-aa41-44e5-966b-12703df4a64a"
      },
      "source": [
        "files = os.listdir(path='data/valid/healthy')\n",
        "nfiles = len(files) + len(os.listdir(\"data/valid/nodule\"))\n",
        "if nfiles % batchsize != 0:\n",
        "  del_files = random.sample(files,nfiles%batchsize)\n",
        "  for dfile in del_files:\n",
        "    os.remove(os.path.join(\"data/valid/healthy/\", dfile))\n",
        "  print(\"Files removed: \", len(del_files))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files removed:  24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soT_7J9A2gm5"
      },
      "source": [
        "## JSRT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QLU_-pL2jOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697462ec-aeb4-497e-a616-425fb503ee3c"
      },
      "source": [
        "total_patches = []\r\n",
        "for j in range(len(jsrt_nodule_list)):\r\n",
        "    image_name = jsrt_nodule_list[j]\r\n",
        "    #print(image_name)\r\n",
        "    bx = jsrt_nodule_info['x'].values[j]/2\r\n",
        "    by = jsrt_nodule_info['y'].values[j]/2\r\n",
        "    box = [bx,by]\r\n",
        "    mask, cxr = read_jsrt(image_name, jsrt_cxr_path, jsrt_mask_path)\r\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'jsrt-nodule', 'data/test/nodule/')\r\n",
        "len(total_patches)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go-B3imW2o6q"
      },
      "source": [
        "total_patches = []\r\n",
        "jsrt_healthy_list = list(jsrt_healthy_list)\r\n",
        "\r\n",
        "for j in range(len(jsrt_healthy_list)):\r\n",
        "    image_name = jsrt_healthy_list[j]\r\n",
        "    print(image_name)\r\n",
        "    box = []\r\n",
        "    mask, cxr = read_jsrt(image_name, jsrt_cxr_path, jsrt_mask_path)\r\n",
        "    total_patches = crop_patches(cxr, mask, total_patches, box, image_name, 'jsrt-healthy', 'data/test/healthy/')\r\n",
        "len(total_patches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM4yo_qd3Z4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2d5899-5e28-42fa-9ce9-b472ed0d18e1"
      },
      "source": [
        "files = os.listdir(path='data/test/healthy')\r\n",
        "nfiles = len(files) + len(os.listdir(\"data/test/nodule\"))\r\n",
        "if nfiles % batchsize != 0:\r\n",
        "  del_files = random.sample(files,nfiles%batchsize)\r\n",
        "  for dfile in del_files:\r\n",
        "    os.remove(os.path.join(\"data/test/healthy/\", dfile))\r\n",
        "  print(\"Files removed: \", len(del_files))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files removed:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS2lz0v7BmJd"
      },
      "source": [
        "# NN Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikmAgOieRDpv"
      },
      "source": [
        "class MyConvo2d(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, he_init=True,  stride=1, bias=True):\n",
        "        super(MyConvo2d, self).__init__()\n",
        "        self.he_init = he_init\n",
        "        self.padding = int((kernel_size - 1)/2)\n",
        "        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size,\n",
        "                              stride=1, padding=self.padding, bias=bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GMjSW7YRG1W"
      },
      "source": [
        "class ConvMeanPool(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, he_init=True):\n",
        "        super(ConvMeanPool, self).__init__()\n",
        "        self.he_init = he_init\n",
        "        self.conv = MyConvo2d(input_dim, output_dim,\n",
        "                              kernel_size, he_init=self.he_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = (output[:, :, ::2, ::2] + output[:, :, 1::2, ::2] +\n",
        "                  output[:, :, ::2, 1::2] + output[:, :, 1::2, 1::2]) / 4\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1kH5oKKRJYn"
      },
      "source": [
        "class MeanPoolConv(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, he_init=True):\n",
        "        super(MeanPoolConv, self).__init__()\n",
        "        self.he_init = he_init\n",
        "        self.conv = MyConvo2d(input_dim, output_dim,\n",
        "                              kernel_size, he_init=self.he_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "        output = (output[:, :, ::2, ::2] + output[:, :, 1::2, ::2] +\n",
        "                  output[:, :, ::2, 1::2] + output[:, :, 1::2, 1::2]) / 4\n",
        "        output = self.conv(output)\n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28YhPArqRMs5"
      },
      "source": [
        "class DepthToSpace(nn.Module):\n",
        "    def __init__(self, block_size):\n",
        "        super(DepthToSpace, self).__init__()\n",
        "        self.block_size = block_size\n",
        "        self.block_size_sq = block_size*block_size\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.permute(0, 2, 3, 1)\n",
        "        (batch_size, input_height, input_width, input_depth) = output.size()\n",
        "        output_depth = int(input_depth / self.block_size_sq)\n",
        "        output_width = int(input_width * self.block_size)\n",
        "        output_height = int(input_height * self.block_size)\n",
        "        t_1 = output.reshape(batch_size, input_height,\n",
        "                             input_width, self.block_size_sq, output_depth)\n",
        "        spl = t_1.split(self.block_size, 3)\n",
        "        stacks = [t_t.reshape(batch_size, input_height,\n",
        "                              output_width, output_depth) for t_t in spl]\n",
        "        output = torch.stack(stacks, 0).transpose(0, 1).permute(0, 2, 1, 3, 4).reshape(\n",
        "            batch_size, output_height, output_width, output_depth)\n",
        "        output = output.permute(0, 3, 1, 2)\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BmPaG7qROvE"
      },
      "source": [
        "class UpSampleConv(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, he_init=True, bias=True):\n",
        "        super(UpSampleConv, self).__init__()\n",
        "        self.he_init = he_init\n",
        "        self.conv = MyConvo2d(input_dim, output_dim,\n",
        "                              kernel_size, he_init=self.he_init, bias=bias)\n",
        "        self.depth_to_space = DepthToSpace(2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "        output = torch.cat((output, output, output, output), 1)\n",
        "        output = self.depth_to_space(output)\n",
        "        output = self.conv(output)\n",
        "        return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rQAeQLGRQgy"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, resample=None, hw=DIM):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.resample = resample\n",
        "        self.bn1 = None\n",
        "        self.bn2 = None\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        if resample == 'down':\n",
        "            self.bn1 = nn.LayerNorm([input_dim, hw, hw])\n",
        "            self.bn2 = nn.LayerNorm([input_dim, hw, hw])\n",
        "        elif resample == 'up':\n",
        "            self.bn1 = nn.BatchNorm2d(input_dim)\n",
        "            self.bn2 = nn.BatchNorm2d(output_dim)\n",
        "        elif resample == 'custom':\n",
        "            self.bn1 = nn.InstanceNorm2d(input_dim)\n",
        "            self.bn2 = nn.InstanceNorm2d(output_dim)\n",
        "        else:\n",
        "            raise Exception('invalid resample value')\n",
        "\n",
        "        if resample == 'down':\n",
        "            self.conv_shortcut = MeanPoolConv(\n",
        "                input_dim, output_dim, kernel_size=1, he_init=False)\n",
        "            self.conv_1 = MyConvo2d(\n",
        "                input_dim, input_dim, kernel_size=kernel_size, bias=False)\n",
        "            self.conv_2 = ConvMeanPool(\n",
        "                input_dim, output_dim, kernel_size=kernel_size)\n",
        "        elif resample == 'up':\n",
        "            self.conv_shortcut = UpSampleConv(\n",
        "                input_dim, output_dim, kernel_size=1, he_init=False)\n",
        "            self.conv_1 = UpSampleConv(\n",
        "                input_dim, output_dim, kernel_size=kernel_size, bias=False)\n",
        "            self.conv_2 = MyConvo2d(\n",
        "                output_dim, output_dim, kernel_size=kernel_size)\n",
        "        elif resample == \"custom\":\n",
        "            self.conv_shortcut = MeanPoolConv(\n",
        "                input_dim, output_dim, kernel_size=1, he_init=False)\n",
        "            self.conv_1 = MyConvo2d(\n",
        "                input_dim, input_dim, kernel_size=kernel_size, bias=False)\n",
        "            self.conv_2 = ConvMeanPool(\n",
        "                input_dim, output_dim, kernel_size=kernel_size)\n",
        "        else:\n",
        "            raise Exception('invalid resample value')\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.input_dim == self.output_dim and self.resample == None:\n",
        "            shortcut = input\n",
        "        else:\n",
        "            shortcut = self.conv_shortcut(input)\n",
        "\n",
        "        output = input\n",
        "        output = self.bn1(output)\n",
        "        output = self.relu1(output)\n",
        "        output = self.conv_1(output)\n",
        "        output = self.bn2(output)\n",
        "        output = self.relu2(output)\n",
        "        output = self.conv_2(output)\n",
        "\n",
        "        return shortcut + output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT-34TZ7RTq7"
      },
      "source": [
        "class ReLULayer(nn.Module):\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(ReLULayer, self).__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.linear = nn.Linear(n_in, n_out)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.linear(input)\n",
        "        output = self.relu(output)\n",
        "        return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqrKQAfCRWQt"
      },
      "source": [
        "class FCGenerator(nn.Module):\n",
        "    def __init__(self, FC_DIM=512):\n",
        "        super(FCGenerator, self).__init__()\n",
        "        self.relulayer1 = ReLULayer(128, FC_DIM)\n",
        "        self.relulayer2 = ReLULayer(FC_DIM, FC_DIM)\n",
        "        self.relulayer3 = ReLULayer(FC_DIM, FC_DIM)\n",
        "        self.relulayer4 = ReLULayer(FC_DIM, FC_DIM)\n",
        "        self.linear = nn.Linear(FC_DIM, OUTPUT_DIM)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.relulayer1(input)\n",
        "        output = self.relulayer2(output)\n",
        "        output = self.relulayer3(output)\n",
        "        output = self.relulayer4(output)\n",
        "        output = self.linear(output)\n",
        "        output = self.tanh(output)\n",
        "        return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F4VC2MgReYU"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim, output_dim, drop_rate=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.conv_in = nn.Conv2d(1, dim, 3, 1, padding=1)\n",
        "        self.res1 = ResidualBlock(dim, dim*2, 3, 'down', DIM)\n",
        "        self.res2 = ResidualBlock(dim*2, dim*4, 3, 'down', int(DIM/2))\n",
        "        self.res3 = ResidualBlock(dim*4, dim*8, 3, 'down', int(DIM/4))\n",
        "        self.res4 = ResidualBlock(dim*8, dim*8, 3, 'down', int(DIM/8))\n",
        "        self.fc = nn.Linear(4*4*8*dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.dropout(x)\n",
        "        x = self.conv_in(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(self.fc(x))\n",
        "        return torch.tanh(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EboMTn_QwP0"
      },
      "source": [
        "from pytorch_msssim import ssim\r\n",
        "\r\n",
        "def ssim_loss(input,target,size_average=True):\r\n",
        "    input = (input + 1) / 2\r\n",
        "    target = (target + 1) / 2\r\n",
        "    ssim_loss = 1 - ssim(input, target, data_range=1, size_average=size_average)\r\n",
        "    return ssim_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJsmaNXBrjx"
      },
      "source": [
        "# Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVXOcHARgTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27c7fdf-b6a1-4675-e3dc-022252c08737"
      },
      "source": [
        "import sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "device = torch.device('cuda:{}'.format(0))\n",
        "torch.cuda.set_device('cuda:{}'.format(0))\n",
        "MODE = 'wgan-gp'  # Valid options are dcgan, wgan, or wgan-gp\n",
        "DIM = 64  # This overfits substantially; you're probably better off with 64\n",
        "LAMBDA = 10  # Gradient penalty lambda hyperparameter\n",
        "CRITIC_ITERS = 5  # How many critic iterations per generator iteration\n",
        "BATCH_SIZE = batchsize  # Batch size\n",
        "ITERS = 100000  # How many generator iterations to train for\n",
        "OUTPUT_DIM = 1 * DIM * DIM  # Number of pixels in image (3*64*64)\n",
        "NOISE_SIZE = 512\n",
        "\n",
        "train_data_path = \"data/train/\"\n",
        "valid_data_path = \"data/valid/\"\n",
        "test_data_path = \"data/test/\"\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.determinstic = False\n",
        "torch.set_deterministic(True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/__init__.py:422: UserWarning: torch.set_deterministic is deprecated and will be removed in a future release. Please use torch.use_deterministic_algorithms instead\n",
            "  \"torch.set_deterministic is deprecated and will be removed in a future \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdXhqA8KRz_G"
      },
      "source": [
        "def calc_gradient_penalty(netD, real_data, fake_data):\n",
        "    alpha = torch.rand(BATCH_SIZE, 1)\n",
        "    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement()/BATCH_SIZE)).contiguous()\n",
        "    alpha = alpha.view(BATCH_SIZE, 1, DIM, DIM)\n",
        "    alpha = alpha.to(device)\n",
        "\n",
        "    fake_data = fake_data.view(BATCH_SIZE, 1, DIM, DIM)\n",
        "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
        "\n",
        "    interpolates = interpolates.to(device)\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    disc_interpolates = netD(interpolates)\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                              grad_outputs=torch.ones(\n",
        "                                  disc_interpolates.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
        "    return gradient_penalty"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux_cOKIaBvIp"
      },
      "source": [
        "# DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37O01eVmMnUH"
      },
      "source": [
        "def get_dataloaders(train_data_path, test_data_path, bs=batchsize):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomAffine(5, translate=[0.05,0.05]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "    ])\n",
        "    \n",
        "    trn_dataset = datasets.ImageFolder(root=train_data_path, transform=test_transform)\n",
        "    tst_dataset = datasets.ImageFolder(root=test_data_path, transform=test_transform)\n",
        "\n",
        "    trn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=bs, shuffle=False)\n",
        "    tst_loader = torch.utils.data.DataLoader(tst_dataset, batch_size=bs, shuffle=False)\n",
        "  \n",
        "    return trn_loader, tst_loader"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIk3JNRV28Fw"
      },
      "source": [
        "# PGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1--0gw512-jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acf8f0b-4756-4b21-e8c5-3651b2aa6264"
      },
      "source": [
        "!git clone https://nitishbhatt56:@github.com/nitishbhatt56/pytorch_GAN_zoo.git"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_GAN_zoo'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 1542 (delta 25), reused 11 (delta 4), pack-reused 1495\u001b[K\n",
            "Receiving objects: 100% (1542/1542), 2.07 MiB | 24.68 MiB/s, done.\n",
            "Resolving deltas: 100% (985/985), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDeyy26C25H_"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/FYDP 2021/PGAN_p64_s4_96k.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjwCiFCW3DMP"
      },
      "source": [
        "import pytorch_GAN_zoo.models.utils.utils as utils\r\n",
        "import json\r\n",
        "import sys\r\n",
        "sys.path.insert(0,'./pytorch_GAN_zoo')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9odjZbo4FcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3f17cc-265c-4783-d3cb-e3886b8c7ec3"
      },
      "source": [
        "checkpt_dir = 'output_networks'\r\n",
        "name='default'\r\n",
        "checkPointDir = os.path.join(checkpt_dir, name)\r\n",
        "module = 'PGAN'\r\n",
        "\r\n",
        "checkpointData = utils.getLastCheckPoint(\r\n",
        "            checkPointDir, name, scale=None, iter=None)\r\n",
        "\r\n",
        "print(checkpointData)\r\n",
        "modelConfig, pathModel, _ = checkpointData\r\n",
        "with open(modelConfig, 'rb') as file:\r\n",
        "    configData = json.load(file)\r\n",
        "\r\n",
        "modelPackage, modelName = utils.getNameAndPackage(module)\r\n",
        "modelType = utils.loadmodule(modelPackage, modelName)\r\n",
        "\r\n",
        "model = modelType(useGPU=True,\r\n",
        "                  storeAVG=True,\r\n",
        "                  **configData)\r\n",
        "\r\n",
        "\r\n",
        "model.load(pathModel)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('output_networks/default/default_train_config.json', 'output_networks/default/default_s4_i96000.pt', 'output_networks/default/default_s4_i96000_tmp_config.json')\n",
            "Average network found !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeeHmkDyB0jW"
      },
      "source": [
        "# Encoder Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0yrXiuw-uuw"
      },
      "source": [
        "def calc_alpha(e):\r\n",
        "  if e < 75:\r\n",
        "      return 0.1\r\n",
        "  return 1"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "C45gpm_tSCAM"
      },
      "source": [
        "def train_encoder():\n",
        "    netG = model.netG\n",
        "    netG.eval()\n",
        "    netD = model.netD\n",
        "    netD.eval()\n",
        "    for p in netD.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in netG.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    dataloader, validloader = get_dataloaders(train_data_path, valid_data_path, BATCH_SIZE)\n",
        "\n",
        "    netE = Encoder(DIM,NOISE_SIZE).to(device)\n",
        "    optimizer = optim.Adam(netE.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
        "    crit = nn.MSELoss()\n",
        "    rec_crit = nn.L1Loss()\n",
        "    \n",
        "    chkpts = os.listdir('encoder/')\n",
        "    if not chkpts: \n",
        "      print('starting from scratch...')\n",
        "      start_iter = 1\n",
        "    else:\n",
        "      print('loading checkpoint...')\n",
        "      chkpts = sorted(chkpts, reverse=True, key=lambda x : float(re.split('_|p',x)[1]))\n",
        "      checkpoint_fpathE = os.path.join('encoder/', chkpts[0])\n",
        "      print(checkpoint_fpathE)\n",
        "      checkpointEl = torch.load(checkpoint_fpathE)\n",
        "\n",
        "      netE.load_state_dict(checkpointEl['state_dict'])\n",
        "      optimizer.load_state_dict(checkpointEl['optimizer'])\n",
        "\n",
        "      start_iter = checkpointEl['epoch']\n",
        "\n",
        "    for e in range(start_iter, 300):\n",
        "        losses = []\n",
        "        netE.train()\n",
        "        options_alpha = .1\n",
        "        for (x, _) in dataloader:\n",
        "            x = x.to(device)\n",
        "            code = netE(x)\n",
        "            rec_image = netG(code)\n",
        "            code_rec = netE(rec_image)\n",
        "            f_x = netD.forward(x,getFeature=True)[1].detach()\n",
        "            # f_x = (f_x - f_x.min(dim=0)[0]) / (f_x.max(dim=0)[0] - f_x.min(dim=0)[0])\n",
        "            f_gx = netD.forward(rec_image,getFeature=True)[1]\n",
        "            # f_gx = (f_gx - f_gx.min(dim=0)[0]) / (f_gx.max(dim=0)[0] - f_gx.min(dim=0)[0])\n",
        "            loss = crit(rec_image, x) + crit(code_rec, code) + options_alpha * crit(f_gx, f_x)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(e, np.mean(losses))\n",
        "        netE.eval()\n",
        "        rec_image = netG(netE(x))\n",
        "        d_input = torch.cat((x, rec_image), dim=0)\n",
        "        save_image(d_input, 'rec'+str(e)+'.jpg',normalize=True)\n",
        "        if e % 1 == 0:\n",
        "            checkpointE = {\n",
        "              'epoch': e + 1,\n",
        "              'state_dict': netE.state_dict(),\n",
        "              'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "            torch.save(checkpointE, 'encoder/netE_%d.pth' % e)\n",
        "            evaluate(netG, netD, netE,options_alpha,validloader)\n",
        "            shutil.copy('encoder/netE_%d.pth' % e,'/content/drive/MyDrive/FYDP 2021/ANOGAN-Weights/PanoGAN/encoder/running/netE_%d.pth' % e)\n",
        "    torch.save(netE.state_dict(), 'wgangp/netE.pth')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YvmUl5VB4vS"
      },
      "source": [
        "# Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZPh4qDKSKkR"
      },
      "source": [
        "def evaluate(netG, netD, netE,options_alpha, dataloader):\n",
        "    options_c = 0\n",
        "    netE.eval()\n",
        "    netD.eval()\n",
        "    netE.eval()\n",
        "\n",
        "    # crit = nn.MSELoss()\n",
        "    y_true, y_score = [], []\n",
        "    rec_scores, enc_scores, feat_scores = [], [], []\n",
        "    in_real, out_real, in_rec, out_rec = [], [], [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (x, label) in dataloader:\n",
        "            bs = x.size(0)\n",
        "            x = x.to(device)\n",
        "            code = netE(x)\n",
        "            rec_image = netG(code)\n",
        "            rec_code = netE(rec_image)\n",
        "\n",
        "            d_input = torch.cat((x, rec_image), dim=0)\n",
        "            idx = (label == options_c)\n",
        "            in_real.append(x[idx])\n",
        "            in_rec.append(rec_image[idx])\n",
        "            idx = (label != options_c)\n",
        "            out_real.append(x[idx])\n",
        "            out_rec.append(rec_image[idx])\n",
        "            # f_x = netD.forward(x,getFeature=True)[1]\n",
        "            # f_gx = netD.forward(rec_image,getFeature=True)[1]\n",
        "            \n",
        "            rec_diff = torch.abs((rec_image.view(bs, -1) - x.view(bs, -1)))\n",
        "            rec_score = rec_diff.mean(dim=1)\n",
        "            # rec_score = ssim_loss(rec_image, x,size_average=False)\n",
        "            rec_scores.append(rec_score.cpu().numpy())\n",
        "\n",
        "            enc_diff = torch.abs((rec_code-code))\n",
        "            #enc_score = enc_diff.mean(dim=1)\n",
        "            enc_scores.append(enc_diff.mean(dim=1).cpu().numpy())\n",
        "\n",
        "            # feat_diff = ((f_x - f_gx)**2)\n",
        "            # feat_score = feat_diff.mean(dim=1) \n",
        "            #feat_scores.append(feat_diff.mean(dim=1).cpu().numpy())\n",
        "\n",
        "            #plt.figure()\n",
        "            #plt.imshow(x[0,0].cpu())\n",
        "            #print(outlier_score.cpu().numpy())\n",
        "            # outlier_score = ssim_loss(rec_image, x, size_average=False)\n",
        "            y_true.append(label)\n",
        "            # y_score.append(outlier_score.cpu())\n",
        "\n",
        "    enc_scores = np.concatenate(enc_scores)\n",
        "    rec_scores = np.concatenate(rec_scores)\n",
        "    #feat_scores = np.concatenate(feat_scores)\n",
        "    rec_scores = (rec_scores - np.min(rec_scores)) / (np.max(rec_scores) - np.min(rec_scores))\n",
        "    #feat_score = (feat_scores - np.min(feat_scores)) / (np.max(feat_scores) - np.min(feat_scores))\n",
        "    enc_scores = (enc_scores - np.min(enc_scores)) / (np.max(enc_scores) - np.min(enc_scores))\n",
        "\n",
        "    y_score = rec_scores + enc_scores #+ feat_scores\n",
        "\n",
        "    in_real = torch.cat(in_real, dim=0)[:32]\n",
        "    in_rec = torch.cat(in_rec, dim=0)[:32]\n",
        "    out_real = torch.cat(out_real, dim=0)[:32]\n",
        "    out_rec = torch.cat(out_rec, dim=0)[:32]\n",
        "    save_image(torch.cat((in_real, in_rec), dim=0), 'healthy.jpg', normalize=True)\n",
        "    save_image(torch.cat((out_real, out_rec), dim=0),\n",
        "               'unhealthy.jpg', normalize=True)\n",
        "    # y_score = np.concatenate(y_score)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_true[y_true != options_c] = -1\n",
        "    y_true[y_true == options_c] = 1\n",
        "    print('auc:', metrics.roc_auc_score(y_true, -y_score))\n",
        "    #plt.figure()\n",
        "    #plt.hist(y_score[y_true==1], 100, density=True, alpha=0.5, color='blue')\n",
        "    #plt.hist(y_score[y_true==-1], 100, density=True, alpha=0.5, color='red')\n",
        "    #plt.show()\n",
        "    return y_true, y_score"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PizW8E4VB66l"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGqUu9xyVyCP"
      },
      "source": [
        "options_eval = False\n",
        "options_stage = 2\n",
        "if not options_eval:\n",
        "    if options_stage == 1:\n",
        "        wgan_training()\n",
        "    elif options_stage == 2:\n",
        "        train_encoder()\n",
        "else:\n",
        "    y_true, y_score = evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpMM_P_Hkm8"
      },
      "source": [
        "!rm -r encoder/\r\n",
        "!mkdir encoder/\r\n",
        "\r\n",
        "for img in os.listdir(\"./\"):\r\n",
        "  if img[-3:] == \"jpg\":\r\n",
        "    os.remove(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_nYBZyF691-"
      },
      "source": [
        "# Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eydudKY66_zB"
      },
      "source": [
        "# load networks\r\n",
        "device = torch.device('cuda:{}'.format(0))\r\n",
        "torch.cuda.set_device('cuda:{}'.format(0))\r\n",
        "checkpointE = torch.load('/content/drive/MyDrive/FYDP 2021/ANOGAN-Weights/PanoGAN/best/best_PANOGAN.pth')\r\n",
        "netE = Encoder(DIM, NOISE_SIZE).to(device)\r\n",
        "netE.load_state_dict(checkpointE['state_dict'])\r\n",
        "\r\n",
        "netE.eval()\r\n",
        "netE.to(device)\r\n",
        "netG = model.avgG\r\n",
        "netG.eval()\r\n",
        "netD = model.netD\r\n",
        "netD.eval()\r\n",
        "for p in netD.parameters():\r\n",
        "    p.requires_grad = False\r\n",
        "for p in netG.parameters():\r\n",
        "    p.requires_grad = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xibCUSG7EqR"
      },
      "source": [
        "def test(dataset):\r\n",
        "    if dataset == \"JSRT\":\r\n",
        "        print(\"Testing JSRT...\\n\")\r\n",
        "        _,dataloader = get_dataloaders(valid_data_path,test_data_path, BATCH_SIZE)\r\n",
        "        fnames = np.array(dataloader.dataset.samples)[:,0]\r\n",
        "        labels = np.array(dataloader.dataset.samples)[:,1].astype(int)\r\n",
        "        nodule_dict = {i.split(\"_\")[2] : [0,0,0,\"\"] for i in fnames[labels==1]}\r\n",
        "    else:\r\n",
        "        print(\"Testing NIH...\\n\")\r\n",
        "        dataloader,_ = get_dataloaders(valid_data_path,test_data_path, BATCH_SIZE)\r\n",
        "        fnames = np.array(dataloader.dataset.samples)[:,0]\r\n",
        "        labels = np.array(dataloader.dataset.samples)[:,1].astype(int)\r\n",
        "        nodule_dict = {i.split(\"_\")[2] + i.split(\"_\")[3] : [0,0,0,\"\"] for i in fnames[labels==1]}\r\n",
        "    y_true, y_score = [], []\r\n",
        "    rec_scores, enc_scores, feat_scores = [], [], []\r\n",
        "    in_real, out_real, in_rec, out_rec = [], [], [], []\r\n",
        "    with torch.no_grad():\r\n",
        "        for j, (x,label) in enumerate(dataloader,0):\r\n",
        "            x = x.to(device)\r\n",
        "            bs = x.size(0)\r\n",
        "            code = netE(x)\r\n",
        "            rec_image = netG(code)\r\n",
        "            rec_code = netE(rec_image)\r\n",
        "            \r\n",
        "            rec_diff = torch.abs((rec_image.view(bs, -1) - x.view(bs, -1)))\r\n",
        "            rec_score = rec_diff.mean(dim=1).cpu().numpy()\r\n",
        "\r\n",
        "            enc_diff = torch.abs((rec_code-code))\r\n",
        "            enc_score = enc_diff.mean(dim=1).cpu().numpy()\r\n",
        "\r\n",
        "            # handle the healthy (h) first\r\n",
        "            h_idx = (label == 0).nonzero(as_tuple=True)[0]\r\n",
        "            h_bs = len(h_idx)\r\n",
        "            if h_bs > 0:\r\n",
        "                in_real.append(x[h_idx])\r\n",
        "                in_rec.append(rec_image[h_idx])\r\n",
        "\r\n",
        "                rec_scores.append(rec_score[h_idx])\r\n",
        "                enc_scores.append(enc_score[h_idx])\r\n",
        "                y_true += + h_bs*[0]\r\n",
        "\r\n",
        "            # handle the unhealthy (u)\r\n",
        "            u_idx = (label == 1).nonzero(as_tuple=True)[0]\r\n",
        "            bs_u = len(u_idx)\r\n",
        "            if bs_u > 0:\r\n",
        "                out_real.append(x[u_idx])\r\n",
        "                out_rec.append(rec_image[u_idx])\r\n",
        "                for idx in u_idx:\r\n",
        "                    fname = fnames[j*BATCH_SIZE + idx]\r\n",
        "                    \r\n",
        "                    if dataset == \"JSRT\": patient_ID = fname.split(\"_\")[2]\r\n",
        "                    else: patient_ID = fname.split(\"_\")[2] + fname.split(\"_\")[3]\r\n",
        "\r\n",
        "                    rec_val = rec_score[idx]\r\n",
        "                    enc_val = enc_score[idx]\r\n",
        "                    total = rec_val + enc_val\r\n",
        "                    # check if it is highest\r\n",
        "                    if nodule_dict[patient_ID][2] < total:\r\n",
        "                        nodule_dict[patient_ID] = [rec_val,enc_val,total,fname]\r\n",
        "\r\n",
        "    rec_scores = np.concatenate(rec_scores)\r\n",
        "    enc_scores = np.concatenate(enc_scores)\r\n",
        "    # retrive the unhealthy scores\r\n",
        "    u_rec_scores = np.array(list(nodule_dict.values()))[:,0]\r\n",
        "    u_enc_scores = np.array(list(nodule_dict.values()))[:,1]\r\n",
        "    # join arrays\r\n",
        "    rec_scores = np.concatenate((rec_scores,u_rec_scores.astype(\"float32\")))\r\n",
        "    enc_scores = np.concatenate((enc_scores,u_enc_scores.astype(\"float32\")))\r\n",
        "    # normalize\r\n",
        "    rec_scores = (rec_scores - np.min(rec_scores)) / (np.max(rec_scores) - np.min(rec_scores))\r\n",
        "    enc_scores = (enc_scores - np.min(enc_scores)) / (np.max(enc_scores) - np.min(enc_scores))\r\n",
        "    # update labels\r\n",
        "    y_true += len(nodule_dict)*[1]\r\n",
        "    y_true = np.array(y_true)\r\n",
        "    y_score = rec_scores + enc_scores\r\n",
        "    # calculate\r\n",
        "    print('auc:', metrics.roc_auc_score(y_true, y_score))\r\n",
        "    plt.figure()\r\n",
        "    plt.hist(y_score[y_true==0], 100, density=True, alpha=0.5, color='blue')\r\n",
        "    plt.hist(y_score[y_true==1], 100, density=True, alpha=0.5, color='red')\r\n",
        "    plt.show()\r\n",
        "    # save images\r\n",
        "    in_real = torch.cat(in_real, dim=0)[:32]\r\n",
        "    in_rec = torch.cat(in_rec, dim=0)[:32]\r\n",
        "    out_real = torch.cat(out_real, dim=0)[:32]\r\n",
        "    out_rec = torch.cat(out_rec, dim=0)[:32]\r\n",
        "    save_image(torch.cat((in_real, in_rec), dim=0), 'test-healthy.jpg', normalize=True)\r\n",
        "    save_image(torch.cat((out_real, out_rec), dim=0),\r\n",
        "               'test-unhealthy.jpg', normalize=True)\r\n",
        "\r\n",
        "    return y_true,y_score,nodule_dict\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "cBhOvigUpFD2",
        "outputId": "6f892048-f286-47e8-e4c8-89b8d44db391"
      },
      "source": [
        "labels,scores,dic= test(\"NIH\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing NIH...\n",
            "\n",
            "auc: 0.8618901853415815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOR0lEQVR4nO3df4xsZ13H8c/He0srSKD1rrGh2G2TRkIJWNigFAJLlXApP6rRPy4BQrHkiloDwUggTXTgH/gL0GhibkojRtOCRU1tbPRqOyHY3Ja99fY3hbZUaUO8S8sPb0iqrV//mGe2Z+fO7JzZnTPnOzPvV7LZM+fHzHefe+5nnn3OM2cdEQIA5PUTbRcAANgZQQ0AyRHUAJAcQQ0AyRHUAJDc/iae9MCBA7G6utrEUwPAQjp+/Pj3ImJl2LZGgnp1dVUbGxtNPDUALCTb/zFqG0MfAJAcQQ0AyRHUAJAcQQ0AyRHUAJAcQQ0AyRHUAJAcQQ0AyRHUAJAcQT3vOp3eF4CFRVADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHK1g9r2Ptv/bvvmJgsCAGw3SY/6w5IebKoQAMBwtYLa9nmS3i7p2mbLAQAMqtuj/rykj0n6v1E72D5se8P2xubm5lSKAwDUCGrb75B0MiKO77RfRByJiLWIWFtZWZlagQCw7Or0qF8v6V22H5N0g6TLbP9Vo1UBALaMDeqI+EREnBcRq5IOSbo1It7beGUAAEnMowaA9PZPsnNEdCV1G6kEADAUPWoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkJvoIOZZQp7P9+26O3e3xACTRowaA9AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEiOoAaA5AhqAEhubFDbPsv2nbbvtn2/7U/OojAAQM/+Gvs8LemyiDhl+wxJX7N9S0Qca7g2AIBqBHVEhKRT5eEZ5SuaLAoA8JxaY9S299k+IemkpKMRcceQfQ7b3rC9sbm5Oe06AWBp1QrqiHg2In5B0nmSXmv7FUP2ORIRaxGxtrKyMu06AWBpTTTrIyJ+IOk2SQebKQcAMKjOrI8V2y8uyz8p6S2SvtF0YQCAnjqzPs6V9EXb+9QL9i9HxM3NlpVLp7P9OwDMUp1ZH/dIumQGtQAAhuCTiQCQHEENAMkR1ACQHEENAMkR1ACQXJ3peRhQnabHlD0ATSOoB6SdM73bd4e0P9AO5rFmoEEE9QiTZgTZAqApjFEDQHIENQAkx9BHwZAFgKzoUQNAcgQ1ACRHUANAcgQ1ACRHUANAcsz6mAAzQwC0gR71lHU6BDqA6SKoASA5ghoAkiOoASA5LibuEePRAJpGjxoAkiOoASA5ghoAkiOoASC5pb6YyIVAAPOAHjUAJEdQA0ByBDUAJEdQA0ByBDUAJLeUsz5mMduj+hrMLgGwF/SoASA5gnoG+GMCAPaCoAaA5MYGte2X2r7N9gO277f94VkUBgDoqXMx8RlJvx8Rd9l+oaTjto9GxAMN17YU+kMiDI0AGGVsjzoivhsRd5Xl/5b0oKSXNF3YItpprJpxbACjTDRGbXtV0iWS7hiy7bDtDdsbm5ub06kOAFB/HrXtn5L0FUkfiYgfDW6PiCOSjkjS2tpaTK3CJcP8awCDagW17TPUC+m/joi/bbak5mQJvix1AJgPdWZ9WNIXJD0YEZ9tviQAQFWdMerXS3qfpMtsnyhflzdcFwCgGDv0ERFfk+QZ1IIBTN0DIPHJRABIj6AGgOSW8janu7He7Wwtd9c7I/dr6vipGjYHcNz4yrh5g3sdn9nN8aNqGvYzMe8Rc4weNQAkR1ADQHIENQAkR1ADQHIENQAkR1DPgU5H6nbbrgJAWwhqAEiOoAaA5Bb+Ay98tgHAvKNHDQDJLXyPehFt+zR0W0UAmBmCeo5szfxYb7EIADNHUM+5fnivt1kEgEYxRg0AyRHUAJAcQQ0AyRHUAJAcQQ0AyS3srA8+kQhgUdCjXhCdDm9OwKIiqAEgOYJ6CXS79LaBeUZQA0ByBDUAJEdQA0ByCzs9b1ltuwVqZ9ReAOYJPWoASI6gBoDkCGoASI6gBoDkCGoASI6gBoDkCOoF1ulU/iAugLk1dh617eskvUPSyYh4RfMl7Q1zhwEsmjo96r+QdLDhOgAAI4wN6oj4qqSnZlALAGCIqY1R2z5se8P2xubm5rSeFgCW3tSCOiKORMRaRKytrKxM62kBYOkx6wMAkuPueUukPyNmvSutr7dYCICJ1Jmed72kdUkHbD8u6Y8i4gtNFzZv1rudoeu766evH7XvXo6vrht2zGnP2y3HDT5eP23X0+3mXqq7mTc5yesM277buZr940YdP277sNcftjzJ9lHPO+m6WVqke+7u9G8yg59zbFBHxLsbeWWksNXLbrMIADtijBoAkiOoASC5hbmYOO9DYAAwCj1qjNTtclMnIIOF6VFjOoYF80QzQgBMHT1qAEiOoAaA5AhqAEhurseomekBYBnQowaA5Oa6R432DN4rBEBz6FEDQHIENQAkR1ADQHKMUWNPtm7T22YRwIKjRw0AyRHUAJAcQx+obac76VW3cfMmYLroUQNAcgQ1GsP9rIHpYOgDUzcYztzPGtibuQxqbsYEYJkw9AEAyRHUAJDcXA59YD4xhQ/YHXrUAJAcQQ0AyRHUaBVzrYHx5mqMmml5AJYRPWqkQy8b2G6uetRYXAQzMBpBjVYQzEB9BDXS63albqe3zHUKLKP0Qc1/zOVVp9fNDZ+wDNIHNVBVfeNe77ZVBTBbzPoAgOToUWMhDBsm6Q+HnHZ/7M5zy50hy5VVQAq1gtr2QUl/LGmfpGsj4jONVgVMwW5nlmyNe0+pDqnyJtCZ4pNiaYwNatv7JP2ZpLdIelzS123fFBEPNFkYJzRmYafzbFi4djq9sXEuXmKW6vSoXyvp4Yh4VJJs3yDpCkmNBDUBjWyGnZPVKYOjbLvYuT7+dfo9+f7zjrpY2u0Mf/PYeqn159YPGvXmMwq/CeTgiNh5B/s3JB2MiA+Wx++T9IsRcfXAfoclHS4Pf17SQxPWckDS9yY8pm3zVjP1Nm/eaqbeZk1S7/kRsTJsw9QuJkbEEUlHdnu87Y2IWJtWPbMwbzVTb/PmrWbqbda06q0zPe8JSS+tPD6vrAMAzECdoP66pItsX2D7eZIOSbqp2bIAAH1jhz4i4hnbV0v6J/Wm510XEfc3UMuuh01aNG81U2/z5q1m6m3WVOodezERANAuPkIOAMkR1ACQ3EyC2vZB2w/Zftj2x4dsP9P2l8r2O2yvVrZ9oqx/yPZbk9T7UdsP2L7H9r/aPr+y7VnbJ8rXTC661qj3Stublbo+WNn2ftvfKl/vn0W9NWv+XKXeb9r+QWVbG218ne2Ttu8bsd22/6T8PPfYfnVl28zbuEa97yl13mv7dtuvqmx7rKw/YXsjSb3rtn9Y+Xf/w8q2Hc+llur9g0qt95Vz9pyybfL2jYhGv9S7APmIpAslPU/S3ZJePrDP70j687J8SNKXyvLLy/5nSrqgPM++BPW+WdLzy/Jv9+stj0813aa7qPdKSX865NhzJD1avp9dls/OUPPA/r+n3kXsVtq4vOYbJb1a0n0jtl8u6RZJlvRLku5ouY3H1Xtpvw5Jb+vXWx4/JulAsvZdl3TzXs+lWdU7sO87Jd26l/adRY966yPoEfE/kvofQa+6QtIXy/KNkn7Ztsv6GyLi6Yj4tqSHy/O1Wm9E3BYRPy4Pj6k3t7wtddp3lLdKOhoRT0XE9yUdlXSwoTqrJq353ZKun0FdI0XEVyU9tcMuV0j6y+g5JunFts9VS208rt6IuL3UI7V/Dtdp31H2cv7v2oT17vn8nUVQv0TSdyqPHy/rhu4TEc9I+qGkn6557LRN+ppXqdeT6jvL9obtY7Z/tYkCB9St99fLr7o32u5/gKmN9p3odcuw0gWSbq2snnUb1zHqZ2qrjScxeA6HpH+2fdy9W0Nk8Trbd9u+xfbFZV3q9rX9fPXemL9SWT1x+3I/6j2w/V5Ja5LeVFl9fkQ8YftCSbfavjciHmmnwi3/IOn6iHja9m+p99vLZS3XVNchSTdGxLOVdRnbeC7ZfrN6Qf2Gyuo3lPb9GUlHbX+j9CDbdJd6/+6nbF8u6e8lXdRyTXW8U9K/RUS19z1x+86iR13nI+hb+9jeL+lFkp6seey01XpN278i6RpJ74qIp/vrI+KJ8v1RSV1JlzRZrGrUGxFPVmq8VtJr6h7bkEle95AGfm1soY3rGPUzpb0Fg+1Xqnc+XBERT/bXV9r3pKS/U/PDjWNFxI8i4lRZ/kdJZ9g+oMTtW+x0/tZv3xkMuu9X7wLKBXpusP/igX1+V9svJn65LF+s7RcTH1XzFxPr1HuJehcwLhpYf7akM8vyAUnfUsMXNmrWe25l+dckHSvL50j6dqn77LJ8ToZzouz3MvUuvLjNNq689qpGX+x6u7ZfTLyzzTauUe/PqXfN59KB9S+Q9MLK8u3q3T2z7Xp/tn8eqBds/1nauta5NOt6y/YXqTeO/YK9tm/jP0wp6HJJ3yzhdk1Z9yn1eqOSdJakvyknzp2SLqwce0057iFJb0tS779I+i9JJ8rXTWX9pZLuLSfLvZKuSlLvpyXdX+q6TdLLKsf+Zmn3hyV9YBb11qm5PO5I+szAcW218fWSvivpf9UbB71K0ockfahst3p/YOORUtdam21co95rJX2/cg5vlPUXlra9u5wz1ySp9+rKOXxMlTeYYedS2/WWfa5UbzJE9bhdtS8fIQeA5PhkIgAkR1ADQHIENQAkR1ADQHIENQAkR1ADQHIENQAk9//AJyheTCe3JwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "NJK35q12McZX",
        "outputId": "d44987ba-a38a-4215-b521-a03f04281b5e"
      },
      "source": [
        "labels,scores,dic= test(\"JSRT\")"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auc: 0.7625482805053064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUIElEQVR4nO3db4xl9X3f8fenywYnsWVs71ReLQuLZdQoRLYhI+J/aid2rWDisI1MlHUbB1ysbVzT2GrUKsQSmfCk8ZM4dYmCVoACjmXjYtdaW6CEBEaOY4E9kOW/SRbilkWojAGDV05o1/32wT0Dl+t7957ZuffOzNn3S7qa8+d3z/ly5vCZ3/7OufekqpAkbX3/ZKMLkCRNhoEuSR1hoEtSRxjoktQRBrokdcQpG7XjHTt21J49ezZq95K0Jd19993fraq5Yes2LND37NnD8vLyRu1ekrakJP9z1DqHXCSpIwx0SeoIA12SOsJAl6SOMNAlqSMMdEnqCANdkjqidaAn2Zbkb5J8dci6U5PclORwkruS7JlkkZKk8dbSQ/8Y8PCIdZcBz1bVG4FPAZ9cb2GSpLVpFehJTgd+Ebh2RJO9wA3N9M3Au5Nk/eWdxBYXey9JaqltD/0Pgf8M/L8R63cBjwNU1THgOeB1g42S7E+ynGR5ZWXlBMqVJI0yNtCTvA94qqruXu/OqupAVc1X1fzc3NDvlpEknaA2PfR3ABcl+Q7weeBdSf50oM0TwG6AJKcArwaenmCdkqQxxgZ6VV1RVadX1R5gH3B7Vf3aQLODwCXN9MVNG58+LUkzdMJfn5vkKmC5qg4C1wGfSXIYeIZe8EuSZmhNgV5VS8BSM31l3/J/BH5lkoVJktbGT4pKUkcY6JLUEQa6JHWEgS5JHWGgS1JHGOiS1BEGuiR1hIEuSR1hoEtSRxjoktQRBrokdYSBLkkdYaBLUkcY6JLUEQa6JHWEgS5JHdHmIdGvSPLNJPcmeTDJ7w1pc2mSlSSHmteHp1OuJGmUNk8segF4V1UdTbId+HqSW6vqzoF2N1XV5ZMvUZLUxthAbx72fLSZ3d68fAC0JG0yrcbQk2xLcgh4Critqu4a0uz9Se5LcnOS3SO2sz/JcpLllZWVdZQtSRrUKtCr6odV9RbgdOD8JD8z0OQrwJ6qehNwG3DDiO0cqKr5qpqfm5tbT92SpAFrusulqr4H3AFcMLD86ap6oZm9FvjZyZQnSWqrzV0uc0lOa6Z/HHgP8O2BNjv7Zi8CHp5kkZKk8drc5bITuCHJNnp/AL5QVV9NchWwXFUHgd9MchFwDHgGuHRaBUuShmtzl8t9wLlDll/ZN30FcMVkS5MkrYWfFJWkjjDQJakjDPQ2Fhdnu69R+zuROo63vVm8X9LMGOiS1BEGuiR1hIEuSR1hoEtSRxjoktQRBrokdYSBLkkdYaBLUkcY6JLUEQa6JHWEgS5JHWGgS1JHGOiS1BEGuiR1RJtnir4iyTeT3JvkwSS/N6TNqUluSnI4yV1J9kyjWEnSaG166C8A76qqNwNvAS5I8taBNpcBz1bVG4FPAZ+cbJmSpHHGBnr1HG1mtzevGmi2F7ihmb4ZeHeSTKxKSdJYrcbQk2xLcgh4Critqu4aaLILeBygqo4BzwGvG7Kd/UmWkyyvrKysr3JJ0su0CvSq+mFVvQU4HTg/yc+cyM6q6kBVzVfV/Nzc3IlsQpI0wprucqmq7wF3ABcMrHoC2A2Q5BTg1cDTkyhQktROm7tc5pKc1kz/OPAe4NsDzQ4ClzTTFwO3V9XgOLskaYpOadFmJ3BDkm30/gB8oaq+muQqYLmqDgLXAZ9Jchh4Btg3tYolSUONDfSqug84d8jyK/um/xH4lcmWtvktLg6flqSN4CdFJakj2gy5qAV765I2moG+Roa1pM3KIRdJ6gh76C0sLcHS4kZXIUnHZw9dkjrCHvoUeIFU0kawhy5JHWGgS1JHGOiS1BEGuiR1hIE+YQtLiyy0ucdx2NXSUVdQFxcnc3V12HYmte312ix1SFuYgS5JHeFtiyP0dxYXNqoISVoDA33KvCdd0qw45CJJHWEPfYZe1lvfqCIkdVabZ4ruTnJHkoeSPJjkY0PaLCR5Lsmh5nXlsG1JkqanTQ/9GPBbVXVPklcBdye5raoeGmj3V1X1vsmXKElqY2wPvaqerKp7munvAw8Du6ZdmCRpbdZ0UTTJHnoPjL5ryOq3Jbk3ya1Jzhnx/v1JlpMsr6ysrLlYSdJorQM9ySuBLwIfr6rnB1bfA5xZVW8G/hvw5WHbqKoDVTVfVfNzc3MnWrMkaYhWgZ5kO70w/2xVfWlwfVU9X1VHm+lbgO1Jdky0UknScbW5yyXAdcDDVfUHI9q8vmlHkvOb7T49yUIlScfX5i6XdwAfBO5PcqhZ9jvAGQBVdQ1wMfCRJMeAfwD2VVVNod7O6H9OqZ8glTQJYwO9qr4OZEybq4GrJ1WUJGnt/Oi/JHWEgS5JHeF3ufRxLFvSVmYPXZI6wkCXpI4w0CWpIwx0SeoIA12SOsJAl6SOMNAlqSMMdEnqCANdkjrCT4puAqufUF1YgoWFDSxE0pZmD12SOsJAl6SOMNAlqSMcQ99klpZemnY8XdJatHmm6O4kdyR5KMmDST42pE2SfDrJ4ST3JTlvOuVKkkZp00M/BvxWVd2T5FXA3Uluq6qH+tq8Fzi7ef0c8MfNT0nSjLR5puiTwJPN9PeTPAzsAvoDfS9wY/Ng6DuTnJZkZ/PezltYWmRpYXHocuDFdavzo9avSf/TOFanB3+2ff+odaPaHG/94qJPCpE2yJouiibZA5wL3DWwahfweN/8kWbZ4Pv3J1lOsryysrK2SiVJx9U60JO8Evgi8PGqev5EdlZVB6pqvqrm5+bmTmQTkqQRWgV6ku30wvyzVfWlIU2eAHb3zZ/eLJMkzUibu1wCXAc8XFV/MKLZQeDXm7td3go8d7KMn0/T0lJvOLr/VkZJGqXNXS7vAD4I3J/kULPsd4AzAKrqGuAW4ELgMPAD4EOTL3U6vH4nqSva3OXydSBj2hTw0UkVJUlaOz/6L0kdYaBLUkcY6JLUEQa6JHWEgS5JHWGgS1JHGOiS1BEG+haxtPTSJ0claRgDXZI6wkCXpI4w0CWpIwx0SeoIA12SOsJAl6SOMNAlqSMMdEnqiDaPoLs+yVNJHhixfiHJc0kONa8rJ1+mJGmcNo+g+xPgauDG47T5q6p630QqkiSdkLE99Kr6GvDMDGqRJK3DpMbQ35bk3iS3JjlnQtuUJK1BmyGXce4Bzqyqo0kuBL4MnD2sYZL9wH6AM844YwK7PjktLsLCUm96YWEDC5G0qay7h15Vz1fV0Wb6FmB7kh0j2h6oqvmqmp+bm1vvriVJfdYd6ElenyTN9PnNNp9e73YlSWszdsglyeeABWBHkiPA7wLbAarqGuBi4CNJjgH/AOyrqppaxZKkocYGelV9YMz6q+nd1rhl+JAISV3kJ0UlqSMMdEnqCANdkjpiEvehn1QWlhZZWljc6DJebthFgeNdKBjVfnB5//yo7W3UBYlh9U7jPdIWYg9dkjrCQJekjnDIZYtbWnpp2q8BkE5u9tAlqSMMdEnqCANdkjrCQJekjjDQJakjDHRJ6ggDXZI6wvvQO2T1nvSlRVjcwDokbQx76JLUEQa6JHXE2EBPcn2Sp5I8MGJ9knw6yeEk9yU5b/JlSpLGadND/xPgguOsfy9wdvPaD/zx+suSJK3V2ECvqq8BzxynyV7gxuq5Ezgtyc5JFShJamcSY+i7gMf75o80y35Ekv1JlpMsr6ysTGDXkqRVM70oWlUHqmq+qubn5uZmuWtJ6rxJ3If+BLC7b/70Zpk2UP/3pAMsbEQRkmZqEj30g8CvN3e7vBV4rqqenMB2JUlrMLaHnuRz9Dp4O5IcAX4X2A5QVdcAtwAXAoeBHwAfmlaxkqTRxgZ6VX1gzPoCPjqxiiRJJ+Sk+S6XxcWNrkCSpsuP/ktSRxjoJ4nFxd6dL4N3v0jqDgNdkjrCQJekjjDQJakjDHRJ6ggDXZI64qS5D10v6b/TZWFho6qQNGn20CWpIwx0SeoIh1xaWlhaHDo9ifbHWz+4bq37XpO1fj/CuPar6/vbLS7+6PsG1w9r16a2Udtu+981rN7NYjPXpk3DHrokdYSBLkkd4ZDLSc47XqTusIcuSR1hoEtSR7QK9CQXJHkkyeEkvz1k/aVJVpIcal4fnnypkqTjafNM0W3AHwHvAY4A30pysKoeGmh6U1VdPoUaJUkttLkoej5wuKoeA0jyeWAvMBjo2uK8QCptbW2GXHYBj/fNH2mWDXp/kvuS3Jxk97ANJdmfZDnJ8srKygmUK0kaZVIXRb8C7KmqNwG3ATcMa1RVB6pqvqrm5+bmJrRrSRK0C/QngP4e9+nNshdV1dNV9UIzey3ws5MpT5LUVpsx9G8BZyc5i16Q7wP+dX+DJDur6slm9iLg4YlWqZlzPF3aesYGelUdS3I58GfANuD6qnowyVXAclUdBH4zyUXAMeAZ4NIp1tya32Mk6WTS6qP/VXULcMvAsiv7pq8ArphsaZKktfCTopLUEX45l8ZyPF3aGgx0rYnhLm1eDrlIUkfYQ9cJs7cubS720CWpIwx0SeoIh1w0ES8bftmoIqSTnIGuiVv9hO7CkmPr0iw55CJJHWEPXVO1tARLi71pv1tHmi576JqZxcVewBvs0nR0roduWEg6WXUu0LU19P/hXRzVSNKaGOjacH7iVJoMA12bymq4Ly32bntctTD7UqQtx0DXlvCyIZrFUa2kk1urQE9yAfBf6T2C7tqq+v2B9acCN9J7OPTTwK9W1XcmW6rUs7jY672v3g754vLZlyJtKmMDPck24I+A9wBHgG8lOVhVD/U1uwx4tqremGQf8EngV6dR8CB7a1rVf8/7qv7gX1hy6Ebd1qaHfj5wuKoeA0jyeWAv0B/oe3mpg3QzcHWSVFVNsNYXGeI6Ucc7d1bH7Af/KIx6f5thIIeKNEsZl7lJLgYuqKoPN/MfBH6uqi7va/NA0+ZIM/9o0+a7A9vaD+xvZv8Z8MgJ1LwD+O7YVrNnXWu3WWuzrrXZrHXB5q1tPXWdWVVzw1bM9KJoVR0ADqxnG0mWq2p+QiVNjHWt3WatzbrWZrPWBZu3tmnV1eaj/08Au/vmT2+WDW2T5BTg1fQujkqSZqRNoH8LODvJWUl+DNgHHBxocxC4pJm+GLh9WuPnkqThxg65VNWxJJcDf0bvtsXrq+rBJFcBy1V1ELgO+EySw8Az9EJ/WtY1ZDNF1rV2m7U261qbzVoXbN7aplLX2IuikqStwa/PlaSOMNAlqSM2TaAnuSDJI0kOJ/ntIetPTXJTs/6uJHv61l3RLH8kyS/MuK7/mOShJPcl+cskZ/at+2GSQ81r8ELyLGq7NMlKXw0f7lt3SZK/a16XDL53ynV9qq+mv03yvb51UztmSa5P8lTzuYlh65Pk003d9yU5r2/dNI/XuLr+TVPP/Um+keTNfeu+0yw/lGR5xnUtJHmu7/d1Zd+6454DU67rP/XV9EBzTr22WTfN47U7yR1NHjyY5GND2kz3HKuqDX/Ru9j6KPAG4MeAe4GfHmjz74Frmul9wE3N9E837U8Fzmq2s22Gdf088BPN9EdW62rmj27wMbsUuHrIe18LPNb8fE0z/ZpZ1TXQ/j/Qu9A+i2P2z4HzgAdGrL8QuBUI8Fbgrmkfr5Z1vX11f8B7V+tq5r8D7Nig47UAfHW958Ck6xpo+0v07rqbxfHaCZzXTL8K+Nsh/09O9RzbLD30F79eoKr+D7D69QL99gI3NNM3A+9Okmb556vqhar6e+Bws72Z1FVVd1TVD5rZO+ndpz8LbY7ZKL8A3FZVz1TVs8BtwAUbVNcHgM9NaN/HVVVfo3cX1ih7gRur507gtCQ7me7xGltXVX2j2S/M8BxrcbxGWc+5Oem6Znl+PVlV9zTT3wceBnYNNJvqObZZAn0X8Hjf/BF+9EC82KaqjgHPAa9r+d5p1tXvMnp/fVe9IslykjuT/KsJ1bTW2t7f/NPu5iSrHxDbFMesGZ46C7i9b/E0j9k4o2qf5vFaq8FzrIA/T3J3el+tMWtvS3JvkluTnNMs2xTHK8lP0AvFL/YtnsnxSm9I+FzgroFVUz3H/D70CUnya8A88C/6Fp9ZVU8keQNwe5L7q+rRGZb1FeBzVfVCkn9H718475rh/sfZB9xcVT/sW7bRx2zTSvLz9AL9nX2L39kcr38K3Jbk200Pdhbuoff7OprkQuDLwNkz2ncbvwT8dVX19+anfrySvJLeH5GPV9Xzk9z2OJulh76erxdo895p1kWSfwl8Arioql5YXV5VTzQ/HwOW6P3FnpSxtVXV0331XEvv++pbvXeadfXZx8A/h6d8zMYZVfs0j1crSd5E73e4t6pe/FqNvuP1FPA/mNxw41hV9XxVHW2mbwG2J9nBJjhejeOdX1M5Xkm20wvzz1bVl4Y0me45No2LAydwMeEUehcBzuKliyjnDLT5KC+/KPqFZvocXn5R9DEmd1G0TV3n0rsAdPbA8tcApzbTO4C/Y7IXhtrUtrNv+peBO+ulCzB/39T4mmb6tbOqq2n3U/QuUGVWx6zZ7h5GX+T7RV5+weqb0z5eLes6g961obcPLP9J4FV909+g962ns6rr9au/P3rB+L+aY9fqHJhWXc36V9MbZ//JWR2v5r/9RuAPj9NmqufYxA7wBA7GhfSuCj8KfKJZdhW9Xi/AK4D/3pzY3wTe0PfeTzTvewR474zr+gvgfwOHmtfBZvnbgfubk/l+4LINOGb/BXiwqeEO4Kf63vtvm2N5GPjQLOtq5heB3x9431SPGb3e2pPA/6U3RnkZ8BvAbzTrQ+9hLo82+5+f0fEaV9e1wLN959hys/wNzbG6t/k9f2LGdV3ed37dSd8fnGHnwKzqatpcSu9mif73Tft4vZPeGP19fb+rC2d5jvnRf0nqiM0yhi5JWicDXZI6wkCXpI4w0CWpIwx0SeoIA12SOsJAl6SO+P+jpW3rRat0gQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}